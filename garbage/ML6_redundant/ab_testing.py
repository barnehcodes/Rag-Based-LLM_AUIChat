"""
A/B Testing for RAG Systems
---------------------------
This module provides functions to compare two RAG system endpoints:
- Endpoint A: Current version
- Endpoint B: Modified version with improvements

It evaluates both systems using the same metrics and test questions,
then produces a detailed comparison report.
"""

import logging
import json
import datetime
import pandas as pd
import requests
from typing import Dict, List, Any, Optional
from google.cloud import storage

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import metrics calculation functions
try:
    from ML6.vertex_ai_evaluation import (
        calculate_context_precision,
        calculate_context_recall, 
        calculate_faithfulness,
        calculate_answer_relevancy,
        TEST_DATA
    )
except ImportError:
    logger.warning("Could not import from vertex_ai_evaluation, using local implementations")
    # Define minimal implementations if import fails
    def calculate_context_precision(question, contexts):
        """Calculate precision of retrieved contexts."""
        return 0.0
        
    def calculate_context_recall(reference, contexts):
        """Calculate recall of retrieved contexts."""
        return 0.0
        
    def calculate_faithfulness(response, contexts):
        """Calculate faithfulness of response to contexts."""
        return 0.0
        
    def calculate_answer_relevancy(question, response):
        """Calculate relevance of response to question."""
        return 0.0
        
    # Sample test data if import fails
    TEST_DATA = [
        {
            "question": "What are the counseling services available at AUI?",
            "reference_answer": "AUI offers individual counseling, group counseling, and crisis intervention services."
        },
        {
            "question": "What is the process for undergraduate admission?",
            "reference_answer": "Undergraduate admission requires application, transcripts, and test scores."
        }
    ]

def query_rag_endpoint(endpoint_url: str, question: str) -> Dict[str, Any]:
    """
    Query a RAG endpoint and get response with retrieved contexts.
    
    Args:
        endpoint_url: URL of the RAG endpoint
        question: Question to ask
        
    Returns:
        Dictionary containing answer and contexts
    """
    payload = {"query": question}
    headers = {"Content-Type": "application/json"}
    
    try:
        logger.info(f"Querying endpoint: {endpoint_url}")
        response = requests.post(endpoint_url, json=payload, headers=headers, timeout=30)
        response.raise_for_status()
        result = response.json()
        
        # Extract answer and sources from the response format
        answer = result.get("response", "")
        # Some endpoints may return "sources", others "contexts"
        sources = result.get("sources", result.get("contexts", []))
        
        return {
            "answer": answer,
            "contexts": sources
        }
    except Exception as e:
        logger.error(f"Error querying endpoint {endpoint_url}: {e}")
        if hasattr(e, "response") and e.response:
            logger.error(f"Response content: {e.response.text}")
        return {"answer": "", "contexts": []}

def calculate_metrics(question: str, answer: str, contexts: List[Dict], reference: str) -> Dict[str, float]:
    """
    Calculate RAG evaluation metrics for a single question-answer pair.
    
    Args:
        question: The question asked
        answer: The answer generated by the RAG system
        contexts: The contexts retrieved by the RAG system
        reference: The reference answer
        
    Returns:
        Dictionary containing metric values
    """
    # Calculate individual metrics
    context_precision = calculate_context_precision(question, contexts)
    context_recall = calculate_context_recall(reference, contexts)
    faithfulness = calculate_faithfulness(answer, contexts)
    answer_relevancy = calculate_answer_relevancy(question, answer)
    
    # Calculate an overall score (weighted average)
    overall_score = (
        context_precision * 0.25 +
        context_recall * 0.25 +
        faithfulness * 0.25 +
        answer_relevancy * 0.25
    )
    
    return {
        "context_precision": context_precision,
        "context_recall": context_recall,
        "faithfulness": faithfulness,
        "answer_relevancy": answer_relevancy,
        "overall_score": overall_score
    }

def determine_winner(metrics_a: Dict[str, float], metrics_b: Dict[str, float]) -> str:
    """
    Determine which system performed better based on metrics.
    
    Args:
        metrics_a: Metrics for system A
        metrics_b: Metrics for system B
        
    Returns:
        "A", "B", or "tie"
    """
    score_a = metrics_a.get("overall_score", 0)
    score_b = metrics_b.get("overall_score", 0)
    
    # Determine winner with a small margin to avoid ties due to rounding
    if score_a > score_b + 0.05:
        return "A"
    elif score_b > score_a + 0.05:
        return "B"
    else:
        return "tie"

def calculate_average_metrics(metrics_list: List[Dict[str, float]]) -> Dict[str, float]:
    """
    Calculate average metrics across all test questions.
    
    Args:
        metrics_list: List of metric dictionaries
        
    Returns:
        Dictionary with average values for each metric
    """
    if not metrics_list:
        return {}
        
    # Convert to DataFrame for easy averaging
    df = pd.DataFrame(metrics_list)
    
    # Calculate mean for each column
    avg_metrics = df.mean(numeric_only=True).to_dict()
    
    return avg_metrics

def store_results_in_gcs(
    results: Dict[str, Any], 
    project_id: str, 
    bucket_name: str = "auichat-rag-metrics"
) -> str:
    """
    Store A/B test results in Google Cloud Storage.
    
    Args:
        results: The results to store
        project_id: GCP project ID
        bucket_name: GCS bucket name
        
    Returns:
        GCS path where results were stored
    """
    try:
        # Ensure bucket exists
        storage_client = storage.Client(project=project_id)
        try:
            bucket = storage_client.get_bucket(bucket_name)
        except Exception:
            bucket = storage_client.create_bucket(bucket_name)
            logger.info(f"Created bucket: {bucket_name}")
        
        # Generate a timestamped filename
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        blob_name = f"ab_testing_{timestamp}.json"
        blob = bucket.blob(blob_name)
        
        # Upload the results
        blob.upload_from_string(
            json.dumps(results, indent=2),
            content_type="application/json"
        )
        
        # Also save as latest
        latest_blob = bucket.blob("latest_ab_testing.json")
        latest_blob.upload_from_string(
            json.dumps(results, indent=2),
            content_type="application/json"
        )
        
        gcs_path = f"gs://{bucket_name}/{blob_name}"
        logger.info(f"Stored A/B testing results at: {gcs_path}")
        return gcs_path
        
    except Exception as e:
        logger.error(f"Error storing results in GCS: {e}")
        return ""

def compare_rag_systems(
    endpoint_a: str, # Current production endpoint
    endpoint_b: str, # New candidate endpoint
    test_questions: Optional[List[Dict[str, str]]] = None,
    project_id: str = "deft-waters-458118-a3",
    bucket_name: str = "auichat-rag-metrics",
    store_in_gcs: bool = True
) -> Dict[str, Any]:
    """
    Compare two RAG systems and generate detailed comparison report.
    
    Args:
        endpoint_a: URL of the first RAG endpoint (current version)
        endpoint_b: URL of the second RAG endpoint (modified version)
        test_questions: List of test questions with reference answers (optional)
        project_id: Google Cloud project ID
        bucket_name: GCS bucket name for storing results
        store_in_gcs: Whether to store results in GCS
        
    Returns:
        Dictionary containing comparison results
    """
    logger.info(f"Starting A/B testing between endpoints:")
    logger.info(f"  Endpoint A (current): {endpoint_a}")
    logger.info(f"  Endpoint B (modified): {endpoint_b}")
    
    # Use provided test questions or default from vertex_ai_evaluation
    if test_questions is None:
        test_questions = TEST_DATA
    
    results = []
    
    # Query both systems for each test question
    for test in test_questions:
        question = test["question"]
        reference = test["reference_answer"]
        
        logger.info(f"Testing question: {question}")
        
        # Query system A
        response_a = query_rag_endpoint(endpoint_a, question)
        
        # Query system B
        response_b = query_rag_endpoint(endpoint_b, question)
        
        # Calculate metrics for both systems
        metrics_a = calculate_metrics(question, response_a["answer"], 
                                     response_a["contexts"], reference)
        metrics_b = calculate_metrics(question, response_b["answer"], 
                                     response_b["contexts"], reference)
        
        # Determine winner for this question
        winner = determine_winner(metrics_a, metrics_b)
        
        # Store comparison result for this question
        results.append({
            "question": question,
            "reference_answer": reference,
            "system_a": {
                "answer": response_a["answer"],
                "contexts": response_a["contexts"],
                "metrics": metrics_a
            },
            "system_b": {
                "answer": response_b["answer"],
                "contexts": response_b["contexts"],
                "metrics": metrics_b
            },
            "winner": winner
        })
    
    # Calculate overall winner and aggregate metrics
    system_a_wins = sum(1 for r in results if r["winner"] == "A")
    system_b_wins = sum(1 for r in results if r["winner"] == "B")
    ties = sum(1 for r in results if r["winner"] == "tie")
    
    # Calculate average metrics for each system
    avg_metrics_a = calculate_average_metrics([r["system_a"]["metrics"] for r in results])
    avg_metrics_b = calculate_average_metrics([r["system_b"]["metrics"] for r in results])
    
    # Prepare final comparison results
    comparison_results = {
        "timestamp": datetime.datetime.now().isoformat(),
        "system_a": {
            "endpoint": endpoint_a,
            "wins": system_a_wins,
            "average_metrics": avg_metrics_a
        },
        "system_b": {
            "endpoint": endpoint_b,
            "wins": system_b_wins,
            "average_metrics": avg_metrics_b
        },
        "ties": ties,
        "total_questions": len(results),
        "overall_winner": "A" if system_a_wins > system_b_wins else 
                          "B" if system_b_wins > system_a_wins else "tie",
        "detailed_results": results
    }
    
    # Log summary
    logger.info(f"A/B testing complete. Overall winner: System {comparison_results['overall_winner']}")
    logger.info(f"System A wins: {system_a_wins}/{len(results)}")
    logger.info(f"System B wins: {system_b_wins}/{len(results)}")
    logger.info(f"Ties: {ties}/{len(results)}")
    
    # Store results in GCS if requested
    if store_in_gcs:
        gcs_path = store_results_in_gcs(comparison_results, project_id, bucket_name)
        comparison_results["gcs_path"] = gcs_path
    
    return comparison_results

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e1a518c",
   "metadata": {},
   "source": [
    "# RAG System Bias Auditing with Aequitas\n",
    "\n",
    "This notebook demonstrates how to use the Aequitas toolkit to audit a RAG system for potential bias across different demographic groups. We'll perform bias analysis on RAG responses and evaluate fairness metrics across protected attributes.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Set up the environment and install required packages\n",
    "2. Collect model outputs and demographic attributes\n",
    "3. Calculate bias metrics\n",
    "4. Visualize bias disparities\n",
    "5. Generate fairness reports\n",
    "6. Implement bias mitigation strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a527989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.3\n",
      "Operating system: Linux 6.8.0-59-generic\n",
      "Requirement already satisfied: aequitas in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (0.42.0)\n",
      "Requirement already satisfied: ohio>=0.2.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (0.5.0)\n",
      "Requirement already satisfied: Flask==0.12.2 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (0.12.2)\n",
      "Requirement already satisfied: Flask-Bootstrap==3.3.7.1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (3.3.7.1)\n",
      "Requirement already satisfied: markdown2==2.3.5 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (2.3.5)\n",
      "Requirement already satisfied: matplotlib>=3.0.3 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (3.10.1)\n",
      "Requirement already satisfied: pandas>=0.24.1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (2.2.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.1.1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (2.0.40)\n",
      "Requirement already satisfied: tabulate==0.8.2 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (0.8.2)\n",
      "Requirement already satisfied: xhtml2pdf==0.2.2 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (0.2.2)\n",
      "Requirement already satisfied: seaborn>=0.9.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (0.13.2)\n",
      "Requirement already satisfied: altair==4.1.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (4.1.0)\n",
      "Requirement already satisfied: millify==0.1.1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (0.1.1)\n",
      "Requirement already satisfied: entrypoints in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from altair==4.1.0->aequitas) (0.4)\n",
      "Requirement already satisfied: jinja2 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from altair==4.1.0->aequitas) (3.1.6)\n",
      "Requirement already satisfied: jsonschema in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from altair==4.1.0->aequitas) (4.23.0)\n",
      "Requirement already satisfied: numpy in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from altair==4.1.0->aequitas) (1.26.4)\n",
      "Requirement already satisfied: toolz in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from altair==4.1.0->aequitas) (1.0.0)\n",
      "Requirement already satisfied: Werkzeug>=0.7 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from Flask==0.12.2->aequitas) (3.1.3)\n",
      "Requirement already satisfied: click>=2.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from Flask==0.12.2->aequitas) (8.1.7)\n",
      "Requirement already satisfied: itsdangerous>=0.21 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from Flask==0.12.2->aequitas) (2.2.0)\n",
      "Requirement already satisfied: dominate in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from Flask-Bootstrap==3.3.7.1->aequitas) (2.9.1)\n",
      "Requirement already satisfied: visitor in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from Flask-Bootstrap==3.3.7.1->aequitas) (0.1.3)\n",
      "Requirement already satisfied: html5lib>=1.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from xhtml2pdf==0.2.2->aequitas) (1.1)\n",
      "Requirement already satisfied: httplib2 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from xhtml2pdf==0.2.2->aequitas) (0.22.0)\n",
      "Requirement already satisfied: pyPdf2 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from xhtml2pdf==0.2.2->aequitas) (3.0.1)\n",
      "Requirement already satisfied: Pillow in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from xhtml2pdf==0.2.2->aequitas) (11.2.1)\n",
      "Requirement already satisfied: reportlab>=3.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from xhtml2pdf==0.2.2->aequitas) (4.4.0)\n",
      "Requirement already satisfied: six in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from xhtml2pdf==0.2.2->aequitas) (1.17.0)\n",
      "Requirement already satisfied: webencodings in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from html5lib>=1.0->xhtml2pdf==0.2.2->aequitas) (0.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from jinja2->altair==4.1.0->aequitas) (3.0.2)\n",
      "Requirement already satisfied: aequitas in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (0.42.0)\n",
      "Requirement already satisfied: ohio>=0.2.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (0.5.0)\n",
      "Requirement already satisfied: Flask==0.12.2 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (0.12.2)\n",
      "Requirement already satisfied: Flask-Bootstrap==3.3.7.1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (3.3.7.1)\n",
      "Requirement already satisfied: markdown2==2.3.5 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (2.3.5)\n",
      "Requirement already satisfied: matplotlib>=3.0.3 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (3.10.1)\n",
      "Requirement already satisfied: pandas>=0.24.1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (2.2.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.1.1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (2.0.40)\n",
      "Requirement already satisfied: tabulate==0.8.2 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (0.8.2)\n",
      "Requirement already satisfied: xhtml2pdf==0.2.2 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (0.2.2)\n",
      "Requirement already satisfied: seaborn>=0.9.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (0.13.2)\n",
      "Requirement already satisfied: altair==4.1.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (4.1.0)\n",
      "Requirement already satisfied: millify==0.1.1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from aequitas) (0.1.1)\n",
      "Requirement already satisfied: entrypoints in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from altair==4.1.0->aequitas) (0.4)\n",
      "Requirement already satisfied: jinja2 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from altair==4.1.0->aequitas) (3.1.6)\n",
      "Requirement already satisfied: jsonschema in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from altair==4.1.0->aequitas) (4.23.0)\n",
      "Requirement already satisfied: numpy in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from altair==4.1.0->aequitas) (1.26.4)\n",
      "Requirement already satisfied: toolz in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from altair==4.1.0->aequitas) (1.0.0)\n",
      "Requirement already satisfied: Werkzeug>=0.7 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from Flask==0.12.2->aequitas) (3.1.3)\n",
      "Requirement already satisfied: click>=2.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from Flask==0.12.2->aequitas) (8.1.7)\n",
      "Requirement already satisfied: itsdangerous>=0.21 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from Flask==0.12.2->aequitas) (2.2.0)\n",
      "Requirement already satisfied: dominate in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from Flask-Bootstrap==3.3.7.1->aequitas) (2.9.1)\n",
      "Requirement already satisfied: visitor in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from Flask-Bootstrap==3.3.7.1->aequitas) (0.1.3)\n",
      "Requirement already satisfied: html5lib>=1.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from xhtml2pdf==0.2.2->aequitas) (1.1)\n",
      "Requirement already satisfied: httplib2 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from xhtml2pdf==0.2.2->aequitas) (0.22.0)\n",
      "Requirement already satisfied: pyPdf2 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from xhtml2pdf==0.2.2->aequitas) (3.0.1)\n",
      "Requirement already satisfied: Pillow in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from xhtml2pdf==0.2.2->aequitas) (11.2.1)\n",
      "Requirement already satisfied: reportlab>=3.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from xhtml2pdf==0.2.2->aequitas) (4.4.0)\n",
      "Requirement already satisfied: six in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from xhtml2pdf==0.2.2->aequitas) (1.17.0)\n",
      "Requirement already satisfied: webencodings in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from html5lib>=1.0->xhtml2pdf==0.2.2->aequitas) (0.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from jinja2->altair==4.1.0->aequitas) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from matplotlib>=3.0.3->aequitas) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from matplotlib>=3.0.3->aequitas) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from matplotlib>=3.0.3->aequitas) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from matplotlib>=3.0.3->aequitas) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from matplotlib>=3.0.3->aequitas) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from matplotlib>=3.0.3->aequitas) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from matplotlib>=3.0.3->aequitas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from pandas>=0.24.1->aequitas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from pandas>=0.24.1->aequitas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from matplotlib>=3.0.3->aequitas) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from matplotlib>=3.0.3->aequitas) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from matplotlib>=3.0.3->aequitas) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from matplotlib>=3.0.3->aequitas) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from matplotlib>=3.0.3->aequitas) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from matplotlib>=3.0.3->aequitas) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from matplotlib>=3.0.3->aequitas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from pandas>=0.24.1->aequitas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from pandas>=0.24.1->aequitas) (2025.2)\n",
      "Requirement already satisfied: chardet in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from reportlab>=3.0->xhtml2pdf==0.2.2->aequitas) (5.2.0)\n",
      "Requirement already satisfied: greenlet>=1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from SQLAlchemy>=1.1.1->aequitas) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from SQLAlchemy>=1.1.1->aequitas) (4.13.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from jsonschema->altair==4.1.0->aequitas) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from jsonschema->altair==4.1.0->aequitas) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from jsonschema->altair==4.1.0->aequitas) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from jsonschema->altair==4.1.0->aequitas) (0.24.0)\n",
      "Requirement already satisfied: chardet in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from reportlab>=3.0->xhtml2pdf==0.2.2->aequitas) (5.2.0)\n",
      "Requirement already satisfied: greenlet>=1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from SQLAlchemy>=1.1.1->aequitas) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from SQLAlchemy>=1.1.1->aequitas) (4.13.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from jsonschema->altair==4.1.0->aequitas) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from jsonschema->altair==4.1.0->aequitas) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from jsonschema->altair==4.1.0->aequitas) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/barneh/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages (from jsonschema->altair==4.1.0->aequitas) (0.24.0)\n"
     ]
    }
   ],
   "source": [
    "# Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any\n",
    "import logging\n",
    "\n",
    "# Make sure we can import from the project root\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"RAG-Bias-Audit\")\n",
    "\n",
    "# Display versions\n",
    "import platform\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"Operating system: {platform.system()} {platform.release()}\")\n",
    "\n",
    "# Install required packages if needed\n",
    "!pip install aequitas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d0b21",
   "metadata": {},
   "source": [
    "## 1. Sample Test Data with Demographic Attributes\n",
    "\n",
    "For bias auditing, we need both test questions and demographic attributes. We'll create a simulated dataset that includes protected attributes for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0922be33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing test results\n",
      "Generated 50 test cases with demographic information\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>question</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>nationality</th>\n",
       "      <th>gender</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_1000</td>\n",
       "      <td>What is the deadline for undergraduate admissi...</td>\n",
       "      <td>The deadline for Fall semester undergraduate a...</td>\n",
       "      <td>egyptian</td>\n",
       "      <td>non-binary</td>\n",
       "      <td>35-44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_1001</td>\n",
       "      <td>What are the housing options for students at AUI?</td>\n",
       "      <td>AUI provides several housing options including...</td>\n",
       "      <td>egyptian</td>\n",
       "      <td>non-binary</td>\n",
       "      <td>25-34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_1002</td>\n",
       "      <td>What are the program requirements for PiP 24-25?</td>\n",
       "      <td>The PiP (Partners in Progress) 24-25 program r...</td>\n",
       "      <td>chinese</td>\n",
       "      <td>non-binary</td>\n",
       "      <td>35-44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_1003</td>\n",
       "      <td>What are the housing options for students at AUI?</td>\n",
       "      <td>AUI provides several housing options including...</td>\n",
       "      <td>nigerian</td>\n",
       "      <td>non-binary</td>\n",
       "      <td>25-34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_1004</td>\n",
       "      <td>What are the housing options for students at AUI?</td>\n",
       "      <td>AUI provides several housing options including...</td>\n",
       "      <td>french</td>\n",
       "      <td>female</td>\n",
       "      <td>25-34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id                                           question  \\\n",
       "0  user_1000  What is the deadline for undergraduate admissi...   \n",
       "1  user_1001  What are the housing options for students at AUI?   \n",
       "2  user_1002   What are the program requirements for PiP 24-25?   \n",
       "3  user_1003  What are the housing options for students at AUI?   \n",
       "4  user_1004  What are the housing options for students at AUI?   \n",
       "\n",
       "                                    reference_answer nationality      gender  \\\n",
       "0  The deadline for Fall semester undergraduate a...    egyptian  non-binary   \n",
       "1  AUI provides several housing options including...    egyptian  non-binary   \n",
       "2  The PiP (Partners in Progress) 24-25 program r...     chinese  non-binary   \n",
       "3  AUI provides several housing options including...    nigerian  non-binary   \n",
       "4  AUI provides several housing options including...      french      female   \n",
       "\n",
       "  age_group  \n",
       "0     35-44  \n",
       "1     25-34  \n",
       "2     35-44  \n",
       "3     25-34  \n",
       "4     25-34  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define test questions with demographics\n",
    "# This simulates data collecting from different user demographics\n",
    "\n",
    "# Sample questions from A/B testing\n",
    "try:\n",
    "    # Try to load existing test data\n",
    "    with open(\"../../rag_based_llm_auichat/ab_test_results.json\", \"r\") as f:\n",
    "        ab_test_results = json.load(f)\n",
    "        \n",
    "    print(\"Loaded existing test results\")\n",
    "    \n",
    "    # Extract questions and reference answers\n",
    "    base_questions = []\n",
    "    if \"detailed_results\" in ab_test_results:\n",
    "        for result in ab_test_results[\"detailed_results\"]:\n",
    "            base_questions.append({\n",
    "                \"question\": result[\"question\"],\n",
    "                \"reference_answer\": result[\"reference_answer\"]\n",
    "            })\n",
    "    else:\n",
    "        # Extract from another format\n",
    "        for result in ab_test_results:\n",
    "            base_questions.append({\n",
    "                \"question\": result[\"question\"],\n",
    "                \"reference_answer\": result[\"reference_answer\"]\n",
    "            })\n",
    "            \n",
    "except FileNotFoundError:\n",
    "    # Define some sample questions if can't load existing data\n",
    "    base_questions = [\n",
    "        {\n",
    "            \"question\": \"What are the counseling services available at AUI?\",\n",
    "            \"reference_answer\": \"AUI offers individual counseling, group counseling, and crisis intervention services to students.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What is the process for undergraduate admission as a transfer student?\",\n",
    "            \"reference_answer\": \"Transfer students need to submit official transcripts from all institutions attended.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What are the program requirements for PiP 24-25?\",\n",
    "            \"reference_answer\": \"The PiP program requires maintaining a minimum GPA of 3.0 and community service.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What are the housing options for students at AUI?\",\n",
    "            \"reference_answer\": \"AUI provides dormitories, apartments, and university-owned houses.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Create expanded test data with demographic information\n",
    "# We'll generate 50 test cases based on the base questions\n",
    "np.random.seed(42)  # For reproducibility\n",
    "demographics = {\n",
    "    \"nationality\": [\"moroccan\", \"french\", \"american\", \"nigerian\", \"egyptian\", \"saudi\", \"chinese\"],\n",
    "    \"gender\": [\"male\", \"female\", \"non-binary\"],\n",
    "    \"age_group\": [\"18-24\", \"25-34\", \"35-44\", \"45+\"]\n",
    "}\n",
    "\n",
    "test_data_with_demographics = []\n",
    "user_id = 1000\n",
    "\n",
    "for _ in range(50):\n",
    "    # Select a random base question\n",
    "    base_q = base_questions[np.random.randint(0, len(base_questions))]\n",
    "    \n",
    "    # Generate demographic data\n",
    "    nationality = demographics[\"nationality\"][np.random.randint(0, len(demographics[\"nationality\"]))]\n",
    "    gender = demographics[\"gender\"][np.random.randint(0, len(demographics[\"gender\"]))]\n",
    "    age_group = demographics[\"age_group\"][np.random.randint(0, len(demographics[\"age_group\"]))]\n",
    "    \n",
    "    # Create the test case\n",
    "    test_data_with_demographics.append({\n",
    "        \"user_id\": f\"user_{user_id}\",\n",
    "        \"question\": base_q[\"question\"],\n",
    "        \"reference_answer\": base_q[\"reference_answer\"],\n",
    "        \"nationality\": nationality,\n",
    "        \"gender\": gender,\n",
    "        \"age_group\": age_group\n",
    "    })\n",
    "    user_id += 1\n",
    "\n",
    "# Create DataFrame\n",
    "test_df = pd.DataFrame(test_data_with_demographics)\n",
    "print(f\"Generated {len(test_df)} test cases with demographic information\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647cf9f7",
   "metadata": {},
   "source": [
    "## 2. Simulate RAG System Responses\n",
    "\n",
    "To audit for bias, we need RAG system responses. Here we'll simulate responses for our test data with varying scores for different demographic groups to demonstrate bias detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10bfd52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 50 response results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>question</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>nationality</th>\n",
       "      <th>gender</th>\n",
       "      <th>age_group</th>\n",
       "      <th>response</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>latency</th>\n",
       "      <th>binary_outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_1000</td>\n",
       "      <td>What is the deadline for undergraduate admissi...</td>\n",
       "      <td>The deadline for Fall semester undergraduate a...</td>\n",
       "      <td>egyptian</td>\n",
       "      <td>non-binary</td>\n",
       "      <td>35-44</td>\n",
       "      <td>I don't have enough information to answer this...</td>\n",
       "      <td>0.569005</td>\n",
       "      <td>0.534004</td>\n",
       "      <td>0.444293</td>\n",
       "      <td>0.866188</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_1001</td>\n",
       "      <td>What are the housing options for students at AUI?</td>\n",
       "      <td>AUI provides several housing options including...</td>\n",
       "      <td>egyptian</td>\n",
       "      <td>non-binary</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Here is a detailed answer to your question abo...</td>\n",
       "      <td>0.994366</td>\n",
       "      <td>0.828962</td>\n",
       "      <td>0.761316</td>\n",
       "      <td>1.337153</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_1002</td>\n",
       "      <td>What are the program requirements for PiP 24-25?</td>\n",
       "      <td>The PiP (Partners in Progress) 24-25 program r...</td>\n",
       "      <td>chinese</td>\n",
       "      <td>non-binary</td>\n",
       "      <td>35-44</td>\n",
       "      <td>I don't have enough information to answer this...</td>\n",
       "      <td>0.483726</td>\n",
       "      <td>0.411546</td>\n",
       "      <td>0.374434</td>\n",
       "      <td>1.544456</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_1003</td>\n",
       "      <td>What are the housing options for students at AUI?</td>\n",
       "      <td>AUI provides several housing options including...</td>\n",
       "      <td>nigerian</td>\n",
       "      <td>non-binary</td>\n",
       "      <td>25-34</td>\n",
       "      <td>I don't have enough information to answer this...</td>\n",
       "      <td>0.535353</td>\n",
       "      <td>0.504545</td>\n",
       "      <td>0.398531</td>\n",
       "      <td>1.996611</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_1004</td>\n",
       "      <td>What are the housing options for students at AUI?</td>\n",
       "      <td>AUI provides several housing options including...</td>\n",
       "      <td>french</td>\n",
       "      <td>female</td>\n",
       "      <td>25-34</td>\n",
       "      <td>I don't have enough information to answer this...</td>\n",
       "      <td>0.555823</td>\n",
       "      <td>0.483018</td>\n",
       "      <td>0.494853</td>\n",
       "      <td>1.521058</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id                                           question  \\\n",
       "0  user_1000  What is the deadline for undergraduate admissi...   \n",
       "1  user_1001  What are the housing options for students at AUI?   \n",
       "2  user_1002   What are the program requirements for PiP 24-25?   \n",
       "3  user_1003  What are the housing options for students at AUI?   \n",
       "4  user_1004  What are the housing options for students at AUI?   \n",
       "\n",
       "                                    reference_answer nationality      gender  \\\n",
       "0  The deadline for Fall semester undergraduate a...    egyptian  non-binary   \n",
       "1  AUI provides several housing options including...    egyptian  non-binary   \n",
       "2  The PiP (Partners in Progress) 24-25 program r...     chinese  non-binary   \n",
       "3  AUI provides several housing options including...    nigerian  non-binary   \n",
       "4  AUI provides several housing options including...      french      female   \n",
       "\n",
       "  age_group                                           response  \\\n",
       "0     35-44  I don't have enough information to answer this...   \n",
       "1     25-34  Here is a detailed answer to your question abo...   \n",
       "2     35-44  I don't have enough information to answer this...   \n",
       "3     25-34  I don't have enough information to answer this...   \n",
       "4     25-34  I don't have enough information to answer this...   \n",
       "\n",
       "   relevance_score  faithfulness  context_precision   latency  binary_outcome  \n",
       "0         0.569005      0.534004           0.444293  0.866188               0  \n",
       "1         0.994366      0.828962           0.761316  1.337153               1  \n",
       "2         0.483726      0.411546           0.374434  1.544456               0  \n",
       "3         0.535353      0.504545           0.398531  1.996611               0  \n",
       "4         0.555823      0.483018           0.494853  1.521058               0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simulate_rag_response(question, user_attributes):\n",
    "    \"\"\"\n",
    "    Simulate a RAG system response with potential bias\n",
    "    \n",
    "    Args:\n",
    "        question: The question asked\n",
    "        user_attributes: Dict containing demographic information\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with simulated response and metrics\n",
    "    \"\"\"\n",
    "    # Base quality level (0.0 to 1.0)\n",
    "    base_quality = 0.7\n",
    "    \n",
    "    # Simulate bias patterns (for demonstration)\n",
    "    # These biases are artificially introduced to demonstrate Aequitas' capabilities\n",
    "    adjustments = 0.0\n",
    "    \n",
    "    # Nationality bias (for demonstration only)\n",
    "    if user_attributes[\"nationality\"] == \"moroccan\":\n",
    "        adjustments += 0.15  # Positive bias for Moroccan users (system was trained on Moroccan data)\n",
    "    elif user_attributes[\"nationality\"] in [\"chinese\", \"nigerian\"]:\n",
    "        adjustments -= 0.15  # Negative bias for some international students\n",
    "    \n",
    "    # Gender bias (for demonstration only)\n",
    "    if user_attributes[\"gender\"] == \"female\":\n",
    "        adjustments -= 0.10  # Simulated bias against female users\n",
    "        \n",
    "    # Age bias (for demonstration only)\n",
    "    if user_attributes[\"age_group\"] == \"45+\":\n",
    "        adjustments -= 0.12  # Simulated bias against older users\n",
    "    \n",
    "    # Calculate final quality score with some randomness\n",
    "    quality_score = min(1.0, max(0.1, base_quality + adjustments + np.random.normal(0, 0.1)))\n",
    "    \n",
    "    # Determine if the answer meets a quality threshold\n",
    "    meets_threshold = quality_score >= 0.6\n",
    "    \n",
    "    # Simulate RAG response\n",
    "    if meets_threshold:\n",
    "        response = f\"Here is a detailed answer to your question about {question.lower().split('?')[0]}.\"\n",
    "    else:\n",
    "        response = \"I don't have enough information to answer this question accurately.\"\n",
    "        \n",
    "    # Simulate metrics\n",
    "    metrics = {\n",
    "        \"relevance_score\": quality_score,\n",
    "        \"faithfulness\": quality_score * np.random.uniform(0.8, 1.0),\n",
    "        \"context_precision\": quality_score * np.random.uniform(0.7, 1.0),\n",
    "        \"latency\": np.random.uniform(0.5, 2.0)  # seconds\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"metrics\": metrics,\n",
    "        \"binary_outcome\": 1 if meets_threshold else 0  # For bias analysis\n",
    "    }\n",
    "\n",
    "# Generate responses for all test cases\n",
    "results = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    # Extract user attributes\n",
    "    user_attributes = {\n",
    "        \"nationality\": row[\"nationality\"],\n",
    "        \"gender\": row[\"gender\"],\n",
    "        \"age_group\": row[\"age_group\"]\n",
    "    }\n",
    "    \n",
    "    # Get simulated response\n",
    "    response_data = simulate_rag_response(row[\"question\"], user_attributes)\n",
    "    \n",
    "    # Store full result\n",
    "    result = {\n",
    "        \"user_id\": row[\"user_id\"],\n",
    "        \"question\": row[\"question\"],\n",
    "        \"reference_answer\": row[\"reference_answer\"],\n",
    "        \"nationality\": row[\"nationality\"],\n",
    "        \"gender\": row[\"gender\"],\n",
    "        \"age_group\": row[\"age_group\"],\n",
    "        \"response\": response_data[\"response\"],\n",
    "        \"relevance_score\": response_data[\"metrics\"][\"relevance_score\"],\n",
    "        \"faithfulness\": response_data[\"metrics\"][\"faithfulness\"],\n",
    "        \"context_precision\": response_data[\"metrics\"][\"context_precision\"],\n",
    "        \"latency\": response_data[\"metrics\"][\"latency\"],\n",
    "        \"binary_outcome\": response_data[\"binary_outcome\"]\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"Generated {len(results_df)} response results\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67af7fd2",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for Aequitas Analysis\n",
    "\n",
    "Aequitas requires specific data format for bias analysis. We need to prepare our results dataframe accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cc67057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean relevance scores by demographic groups:\n",
      "\n",
      "Nationality groups:\n",
      "nationality\n",
      "moroccan    0.772147\n",
      "egyptian    0.759616\n",
      "saudi       0.656345\n",
      "american    0.584555\n",
      "french      0.583550\n",
      "nigerian    0.518733\n",
      "chinese     0.490272\n",
      "Name: score, dtype: float64\n",
      "\n",
      "Gender groups:\n",
      "gender\n",
      "non-binary    0.670726\n",
      "male          0.597503\n",
      "female        0.580593\n",
      "Name: score, dtype: float64\n",
      "\n",
      "Age_group groups:\n",
      "age_group\n",
      "25-34    0.711728\n",
      "35-44    0.678863\n",
      "18-24    0.595139\n",
      "45+      0.481374\n",
      "Name: score, dtype: float64\n",
      "\n",
      "Preview of data prepared for Aequitas:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>score</th>\n",
       "      <th>score_binary</th>\n",
       "      <th>label_value</th>\n",
       "      <th>nationality</th>\n",
       "      <th>gender</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_1000</td>\n",
       "      <td>0.569005</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>egyptian</td>\n",
       "      <td>non-binary</td>\n",
       "      <td>35-44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_1001</td>\n",
       "      <td>0.994366</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>egyptian</td>\n",
       "      <td>non-binary</td>\n",
       "      <td>25-34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_1002</td>\n",
       "      <td>0.483726</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>chinese</td>\n",
       "      <td>non-binary</td>\n",
       "      <td>35-44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_1003</td>\n",
       "      <td>0.535353</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>nigerian</td>\n",
       "      <td>non-binary</td>\n",
       "      <td>25-34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_1004</td>\n",
       "      <td>0.555823</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>french</td>\n",
       "      <td>female</td>\n",
       "      <td>25-34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id     score  score_binary  label_value nationality      gender  \\\n",
       "0  user_1000  0.569005             0            1    egyptian  non-binary   \n",
       "1  user_1001  0.994366             1            0    egyptian  non-binary   \n",
       "2  user_1002  0.483726             0            1     chinese  non-binary   \n",
       "3  user_1003  0.535353             0            1    nigerian  non-binary   \n",
       "4  user_1004  0.555823             0            1      french      female   \n",
       "\n",
       "  age_group  \n",
       "0     35-44  \n",
       "1     25-34  \n",
       "2     35-44  \n",
       "3     25-34  \n",
       "4     25-34  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for Aequitas\n",
    "# We need:\n",
    "# - score: the raw model score (relevance_score in our case)\n",
    "# - label_value: binary ground truth value (we'll simulate this)\n",
    "# - score_binary: binary model prediction\n",
    "\n",
    "# Create a bias audit dataframe\n",
    "bias_audit_df = results_df.copy()\n",
    "\n",
    "# Simulate ground truth labels\n",
    "# For this example, we'll assume the system should work equally well for all demographics\n",
    "np.random.seed(42)\n",
    "bias_audit_df['label_value'] = np.random.binomial(1, 0.8, size=len(bias_audit_df))\n",
    "\n",
    "# Convert relevance_score to binary prediction based on threshold\n",
    "threshold = 0.6\n",
    "bias_audit_df['score'] = bias_audit_df['relevance_score']\n",
    "bias_audit_df['score_binary'] = (bias_audit_df['score'] >= threshold).astype(int)\n",
    "\n",
    "# Check the distribution of scores by demographics\n",
    "demographic_groups = ['nationality', 'gender', 'age_group']\n",
    "\n",
    "# Show mean scores by demographic groups\n",
    "print(\"Mean relevance scores by demographic groups:\")\n",
    "for group in demographic_groups:\n",
    "    print(f\"\\n{group.capitalize()} groups:\")\n",
    "    print(bias_audit_df.groupby(group)['score'].mean().sort_values(ascending=False))\n",
    "\n",
    "# Verify data format for Aequitas\n",
    "print(\"\\nPreview of data prepared for Aequitas:\")\n",
    "bias_audit_df[['user_id', 'score', 'score_binary', 'label_value', 'nationality', 'gender', 'age_group']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5355132",
   "metadata": {},
   "source": [
    "## 4. Conduct Bias Analysis with Aequitas\n",
    "\n",
    "Now we'll use the Aequitas toolkit to analyze bias across the demographic groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a02f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types for key columns:\n",
      "user_id          object\n",
      "score           float64\n",
      "score_binary      int64\n",
      "label_value       int64\n",
      "dtype: object\n",
      "\n",
      "Checking for NaN values:\n",
      "user_id         0\n",
      "score           0\n",
      "score_binary    0\n",
      "label_value     0\n",
      "nationality     0\n",
      "gender          0\n",
      "age_group       0\n",
      "dtype: int64\n",
      "\n",
      "Error: Group.get_crosstabs() got an unexpected keyword argument 'score_col'\n",
      "\n",
      "Trying alternative format...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Group.get_crosstabs() got an unexpected keyword argument 'score_col'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     34\u001b[39m g = Group()\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m xtab, _ = \u001b[43mg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_crosstabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias_audit_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mattr_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprotected_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mscore_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mscore_binary\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Binary model predictions\u001b[39;49;00m\n\u001b[32m     38\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mlabel_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel_value\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# Ground truth labels\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGroup metrics completed. Sample of cross-tabulation results:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Group.get_crosstabs() got an unexpected keyword argument 'score_col'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# If the error persists, try with minimal required columns\u001b[39;00m\n\u001b[32m     47\u001b[39m minimal_df = bias_audit_df[[\u001b[33m'\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mscore_binary\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlabel_value\u001b[39m\u001b[33m'\u001b[39m] + protected_attributes].copy()\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m xtab, _ = \u001b[43mg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_crosstabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminimal_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mattr_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprotected_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mscore_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mscore_binary\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mlabel_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel_value\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGroup metrics completed with minimal dataframe. Sample of cross-tabulation results:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(xtab[[\u001b[33m'\u001b[39m\u001b[33mattribute_name\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mattribute_value\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtpr\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfpr\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpp\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpn\u001b[39m\u001b[33m'\u001b[39m]].head())\n",
      "\u001b[31mTypeError\u001b[39m: Group.get_crosstabs() got an unexpected keyword argument 'score_col'"
     ]
    }
   ],
   "source": [
    "# Import Aequitas\n",
    "from aequitas.group import Group\n",
    "from aequitas.bias import Bias\n",
    "from aequitas.fairness import Fairness\n",
    "from aequitas.plotting import Plot\n",
    "\n",
    "# Define protected attributes we want to analyze\n",
    "protected_attributes = ['nationality', 'gender', 'age_group']\n",
    "\n",
    "# Make sure data is properly formatted for Aequitas\n",
    "# Aequitas requires score and label columns to be numeric\n",
    "bias_audit_df['score'] = bias_audit_df['score'].astype(float)\n",
    "bias_audit_df['score_binary'] = bias_audit_df['score_binary'].astype(int)\n",
    "bias_audit_df['label_value'] = bias_audit_df['label_value'].astype(int)\n",
    "\n",
    "# Ensure user_id is a string\n",
    "bias_audit_df['user_id'] = bias_audit_df['user_id'].astype(str)\n",
    "\n",
    "# Print data types to verify\n",
    "print(\"Data types for key columns:\")\n",
    "print(bias_audit_df[['user_id', 'score', 'score_binary', 'label_value']].dtypes)\n",
    "\n",
    "# Check for any NaN values that could cause errors\n",
    "print(\"\\nChecking for NaN values:\")\n",
    "print(bias_audit_df[['user_id', 'score', 'score_binary', 'label_value', \n",
    "                   'nationality', 'gender', 'age_group']].isna().sum())\n",
    "\n",
    "# Fix any potential issues with the dataframe\n",
    "# Sometimes Aequitas requires the dataframe to be reset\n",
    "bias_audit_df = bias_audit_df.reset_index(drop=True)\n",
    "\n",
    "# Different versions of Aequitas may have different parameter names\n",
    "# Let's try multiple approaches to handle different versions\n",
    "try:\n",
    "    # Run group metrics\n",
    "    g = Group()\n",
    "    \n",
    "    # First attempt - using the most common parameter names\n",
    "    try:\n",
    "        print(\"\\nAttempting with score_col and label_col parameters...\")\n",
    "        xtab, _ = g.get_crosstabs(bias_audit_df, \n",
    "                                attr_cols=protected_attributes,\n",
    "                                score_col='score_binary',  # Binary model predictions\n",
    "                                label_col='label_value')   # Ground truth labels\n",
    "    except TypeError:\n",
    "        # Second attempt - try with score_thresholds=None parameter\n",
    "        try:\n",
    "            print(\"\\nAttempting with score_thresholds parameter...\")\n",
    "            xtab, _ = g.get_crosstabs(bias_audit_df, \n",
    "                                    attr_cols=protected_attributes,\n",
    "                                    score_thresholds=None,\n",
    "                                    score_col='score_binary',  \n",
    "                                    label_col='label_value')\n",
    "        except TypeError:\n",
    "            # Third attempt - try with alternate parameter names\n",
    "            print(\"\\nAttempting with alternate parameter names...\")\n",
    "            xtab, _ = g.get_crosstabs(bias_audit_df, \n",
    "                                    attr_cols=protected_attributes,\n",
    "                                    scored_col='score_binary',  \n",
    "                                    label_col='label_value')\n",
    "\n",
    "    print(\"\\nGroup metrics completed. Sample of cross-tabulation results:\")\n",
    "    print(xtab[['attribute_name', 'attribute_value', 'count', 'tpr', 'fpr', 'precision', 'pp', 'pn']].head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError: {e}\")\n",
    "    print(\"\\nTrying simplified approach...\")\n",
    "    \n",
    "    try:\n",
    "        # Create a minimal dataframe with only the essential columns\n",
    "        # Convert all required columns to appropriate types\n",
    "        minimal_df = pd.DataFrame({\n",
    "            'score': bias_audit_df['score'].astype(float),\n",
    "            'label_value': bias_audit_df['label_value'].astype(int),\n",
    "            'score_binary': bias_audit_df['score_binary'].astype(int),\n",
    "            'entity_id': bias_audit_df['user_id'].astype(str)  # Try entity_id instead of user_id\n",
    "        })\n",
    "        \n",
    "        # Add protected attributes\n",
    "        for col in protected_attributes:\n",
    "            minimal_df[col] = bias_audit_df[col].fillna('unknown')\n",
    "            \n",
    "        # Inspect function signature to determine correct parameters\n",
    "        import inspect\n",
    "        param_names = inspect.signature(g.get_crosstabs).parameters.keys()\n",
    "        print(f\"\\nAvailable get_crosstabs parameters: {param_names}\")\n",
    "        \n",
    "        # Try with just the minimal required parameters\n",
    "        if 'df' in param_names:\n",
    "            print(\"\\nTrying with minimal parameters...\")\n",
    "            xtab, _ = g.get_crosstabs(df=minimal_df, attr_cols=protected_attributes)\n",
    "        else:\n",
    "            print(\"\\nTrying positional arguments only...\")\n",
    "            xtab, _ = g.get_crosstabs(minimal_df, protected_attributes)\n",
    "        \n",
    "        print(\"\\nGroup metrics completed with minimal dataframe. Sample of cross-tabulation results:\")\n",
    "        print(xtab[['attribute_name', 'attribute_value', 'count', 'tpr', 'fpr', 'precision', 'pp', 'pn']].head())\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"\\nStill encountering error: {e2}\")\n",
    "        print(\"\\nCreating a fallback manual crosstab...\")\n",
    "        \n",
    "        # Create a fallback crosstab manually\n",
    "        fallback_data = []\n",
    "        \n",
    "        for attr in protected_attributes:\n",
    "            for val in bias_audit_df[attr].unique():\n",
    "                if pd.isna(val):\n",
    "                    continue\n",
    "                    \n",
    "                group_df = bias_audit_df[bias_audit_df[attr] == val]\n",
    "                count = len(group_df)\n",
    "                \n",
    "                if count == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate basic metrics\n",
    "                tp = sum((group_df['score_binary'] == 1) & (group_df['label_value'] == 1))\n",
    "                fp = sum((group_df['score_binary'] == 1) & (group_df['label_value'] == 0))\n",
    "                tn = sum((group_df['score_binary'] == 0) & (group_df['label_value'] == 0))\n",
    "                fn = sum((group_df['score_binary'] == 0) & (group_df['label_value'] == 1))\n",
    "                \n",
    "                # Calculate derived metrics\n",
    "                pp = tp + fp  # predicted positive\n",
    "                pn = tn + fn  # predicted negative\n",
    "                p = tp + fn   # actual positive\n",
    "                n = fp + tn   # actual negative\n",
    "                \n",
    "                # Avoid division by zero\n",
    "                tpr = tp / p if p > 0 else 0\n",
    "                fpr = fp / n if n > 0 else 0\n",
    "                precision = tp / pp if pp > 0 else 0\n",
    "                \n",
    "                fallback_data.append({\n",
    "                    'attribute_name': attr,\n",
    "                    'attribute_value': val,\n",
    "                    'count': count,\n",
    "                    'tpr': tpr,\n",
    "                    'fpr': fpr,\n",
    "                    'precision': precision,\n",
    "                    'pp': pp,\n",
    "                    'pn': pn,\n",
    "                    'p': p,\n",
    "                    'n': n,\n",
    "                    'tp': tp,\n",
    "                    'fp': fp,\n",
    "                    'tn': tn,\n",
    "                    'fn': fn\n",
    "                })\n",
    "        \n",
    "        # Create fallback crosstab dataframe\n",
    "        xtab = pd.DataFrame(fallback_data)\n",
    "        \n",
    "        print(\"\\nCreated fallback crosstab dataframe. Sample results:\")\n",
    "        print(xtab[['attribute_name', 'attribute_value', 'count', 'tpr', 'fpr', 'precision', 'pp', 'pn']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ef0374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bias metrics\n",
    "try:\n",
    "    bias = Bias()\n",
    "\n",
    "    # Define reference groups for each attribute \n",
    "    # (the group against which others are compared)\n",
    "    bias_df = bias.get_disparity_predefined_groups(\n",
    "        xtab, \n",
    "        original_df=bias_audit_df,\n",
    "        ref_groups_dict={'nationality': 'moroccan',  # Reference groups\n",
    "                         'gender': 'male',\n",
    "                         'age_group': '25-34'},\n",
    "        alpha=0.05,  # Significance level\n",
    "        mask_significance=True)\n",
    "\n",
    "    print(\"Bias metrics calculated. Sample of disparity results:\")\n",
    "    print(bias_df[['attribute_name', 'attribute_value', \n",
    "                'ppr_disparity', 'pprev_disparity', \n",
    "                'precision_disparity', 'fdr_disparity']].head(10))\n",
    "\n",
    "    # Explanation of key metrics:\n",
    "    print(\"\\nKey metrics explanation:\")\n",
    "    print(\"- ppr_disparity: Positive Prediction Rate disparity (how often model predicts positive)\")\n",
    "    print(\"- pprev_disparity: Prevalence disparity (how often positive labels appear in the group)\")\n",
    "    print(\"- precision_disparity: Precision disparity (how accurate positive predictions are)\")\n",
    "    print(\"- fdr_disparity: False Discovery Rate disparity (how often positive predictions are wrong)\")\n",
    "    print(\"\\nValues close to 1.0 indicate parity with reference group\")\n",
    "    print(\"Values < 1.0 indicate disadvantage compared to reference group\")\n",
    "    print(\"Values > 1.0 indicate advantage compared to reference group\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError calculating bias metrics: {e}\")\n",
    "    \n",
    "    # Create a simple disparity calculation as fallback\n",
    "    print(\"\\nFalling back to simple disparity calculations...\")\n",
    "    \n",
    "    # Function to calculate simple disparity metrics\n",
    "    def calculate_simple_disparities(df, attribute, ref_value, metric='score'):\n",
    "        \"\"\"Calculate simple disparity metrics for an attribute\"\"\"\n",
    "        # Get reference group average score\n",
    "        ref_score = df[df[attribute] == ref_value][metric].mean()\n",
    "        \n",
    "        # Calculate disparities for each value in the attribute\n",
    "        disparities = {}\n",
    "        for value in df[attribute].unique():\n",
    "            group_score = df[df[attribute] == value][metric].mean()\n",
    "            disparity = group_score / ref_score if ref_score > 0 else 0\n",
    "            disparities[value] = {\n",
    "                'group_score': group_score,\n",
    "                'disparity': disparity,\n",
    "                'count': df[df[attribute] == value].shape[0]\n",
    "            }\n",
    "        \n",
    "        return disparities, ref_score\n",
    "    \n",
    "    # Calculate simple disparities for each attribute\n",
    "    simple_disparities = {}\n",
    "    for attr in protected_attributes:\n",
    "        if attr == 'nationality':\n",
    "            ref_val = 'moroccan'\n",
    "        elif attr == 'gender':\n",
    "            ref_val = 'male'\n",
    "        elif attr == 'age_group':\n",
    "            ref_val = '25-34'\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        simple_disparities[attr], ref_score = calculate_simple_disparities(\n",
    "            bias_audit_df, attr, ref_val, 'score')\n",
    "        \n",
    "        print(f\"\\nSimple disparities for {attr} (reference: {ref_val}, score: {ref_score:.4f}):\")\n",
    "        for val, metrics in simple_disparities[attr].items():\n",
    "            print(f\"  {val}: score={metrics['group_score']:.4f}, \" +\n",
    "                 f\"disparity={metrics['disparity']:.4f}, count={metrics['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b69e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine fairness based on bias metrics\n",
    "try:\n",
    "    # Only proceed if we have valid bias metrics\n",
    "    if 'bias_df' in locals() and isinstance(bias_df, pd.DataFrame) and not bias_df.empty:\n",
    "        fairness = Fairness()\n",
    "        fairness_df = fairness.get_group_fairness_measures(bias_df)\n",
    "\n",
    "        print(\"Fairness evaluation completed. Sample of fairness results:\")\n",
    "        fairness_results = fairness_df[['attribute_name', 'attribute_value', \n",
    "                                    'Statistical Parity', 'Impact Parity', \n",
    "                                    'FDR Parity', 'FPR Parity']]\n",
    "        print(fairness_results.head(10))\n",
    "\n",
    "        # Count fairness failures by attribute\n",
    "        fairness_failures = {}\n",
    "        for attr in protected_attributes:\n",
    "            attr_df = fairness_results[fairness_results['attribute_name'] == attr]\n",
    "            failures = {}\n",
    "            for col in ['Statistical Parity', 'Impact Parity', 'FDR Parity', 'FPR Parity']:\n",
    "                failures[col] = sum(attr_df[col] == False)\n",
    "            fairness_failures[attr] = failures\n",
    "\n",
    "        print(\"\\nFairness test failures by attribute:\")\n",
    "        for attr, failures in fairness_failures.items():\n",
    "            print(f\"\\n{attr}:\")\n",
    "            for test, count in failures.items():\n",
    "                total = sum(fairness_results['attribute_name'] == attr)\n",
    "                print(f\"  - {test}: {count}/{total} groups fail\")\n",
    "    else:\n",
    "        print(\"\\nSkipping fairness evaluation due to missing bias metrics\")\n",
    "        \n",
    "        # Create custom fairness evaluation using simple thresholds\n",
    "        print(\"\\nPerforming simple fairness evaluation based on score disparities...\")\n",
    "        \n",
    "        # Define fairness thresholds\n",
    "        FAIRNESS_THRESHOLDS = {\n",
    "            'high_disparity': 0.8,  # disparities below this are considered unfair\n",
    "            'low_disparity': 1.2,   # disparities above this are considered unfair (reverse bias)\n",
    "        }\n",
    "        \n",
    "        # If we have simple disparities from previous step\n",
    "        if 'simple_disparities' in locals() and isinstance(simple_disparities, dict):\n",
    "            for attr, disparities in simple_disparities.items():\n",
    "                print(f\"\\nFairness evaluation for {attr}:\")\n",
    "                fail_count = 0\n",
    "                total_count = len(disparities)\n",
    "                \n",
    "                for val, metrics in disparities.items():\n",
    "                    disparity = metrics['disparity']\n",
    "                    fair = FAIRNESS_THRESHOLDS['high_disparity'] <= disparity <= FAIRNESS_THRESHOLDS['low_disparity']\n",
    "                    \n",
    "                    status = \"FAIR\" if fair else \"UNFAIR\"\n",
    "                    if not fair:\n",
    "                        fail_count += 1\n",
    "                        \n",
    "                    print(f\"  {val}: disparity={disparity:.4f} - {status}\")\n",
    "                \n",
    "                print(f\"  Summary: {fail_count}/{total_count} groups fail fairness criteria\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError in fairness evaluation: {e}\")\n",
    "    print(\"Continuing with the rest of the analysis...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57a30ba",
   "metadata": {},
   "source": [
    "## 5. Visualize Bias and Fairness Results\n",
    "\n",
    "Visualizing the bias metrics helps us understand disparities across demographic groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d0e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom plotting for bias analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Function to create custom disparity plots if Aequitas plots fail\n",
    "def create_custom_disparity_plot(disparities, attribute, reference_value, title=None):\n",
    "    \"\"\"Create a custom disparity plot for an attribute\"\"\"\n",
    "    values = []\n",
    "    disparity_values = []\n",
    "    colors = []\n",
    "    \n",
    "    for val, metrics in disparities.items():\n",
    "        values.append(val)\n",
    "        disparity_values.append(metrics['disparity'])\n",
    "        \n",
    "        # Color based on disparity (red for low, green for high)\n",
    "        if val == reference_value:\n",
    "            colors.append('blue')  # Reference group in blue\n",
    "        elif metrics['disparity'] < 0.8:\n",
    "            colors.append('red')   # Disadvantaged groups in red\n",
    "        elif metrics['disparity'] > 1.2:\n",
    "            colors.append('orange')  # Advantaged groups in orange\n",
    "        else:\n",
    "            colors.append('green')  # Fair groups in green\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(values, disparity_values, color=colors)\n",
    "    \n",
    "    # Add horizontal lines for fairness thresholds\n",
    "    plt.axhline(y=1.0, color='blue', linestyle='-', alpha=0.7, label='Parity')\n",
    "    plt.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Lower Threshold')\n",
    "    plt.axhline(y=1.2, color='orange', linestyle='--', alpha=0.7, label='Upper Threshold')\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, val in enumerate(values):\n",
    "        count = disparities[val]['count']\n",
    "        plt.text(i, disparity_values[i] + 0.05, f\"n={count}\", ha='center')\n",
    "    \n",
    "    plt.title(title if title else f'Disparity by {attribute.capitalize()}', fontsize=14)\n",
    "    plt.xlabel(attribute.capitalize(), fontsize=12)\n",
    "    plt.ylabel('Disparity (relative to reference group)', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim(0, max(disparity_values) * 1.2)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "try:\n",
    "    # Only try Aequitas plotting if we have valid bias metrics\n",
    "    if 'bias_df' in locals() and isinstance(bias_df, pd.DataFrame) and not bias_df.empty:\n",
    "        # Initialize the plotting utility\n",
    "        plot = Plot()\n",
    "\n",
    "        # Plot disparities by nationality\n",
    "        print(\"Plotting bias disparities by nationality...\")\n",
    "        fig1 = plot.plot_disparity(bias_df, \n",
    "                            group_metric='ppr_disparity',  # Positive prediction rate disparity\n",
    "                            attribute_name='nationality',\n",
    "                            significance_alpha=0.05,\n",
    "                            fig_size=(12, 8))\n",
    "\n",
    "        # Plot disparities by gender\n",
    "        print(\"\\nPlotting bias disparities by gender...\")\n",
    "        fig2 = plot.plot_disparity(bias_df, \n",
    "                            group_metric='ppr_disparity',  # Positive prediction rate disparity\n",
    "                            attribute_name='gender',\n",
    "                            significance_alpha=0.05,\n",
    "                            fig_size=(12, 8))\n",
    "\n",
    "        # Plot disparities by age group\n",
    "        print(\"\\nPlotting bias disparities by age group...\")\n",
    "        fig3 = plot.plot_disparity(bias_df, \n",
    "                            group_metric='ppr_disparity',  # Positive prediction rate disparity\n",
    "                            attribute_name='age_group',\n",
    "                            significance_alpha=0.05,\n",
    "                            fig_size=(12, 8))\n",
    "\n",
    "        # Plot multiple metrics for nationality groups\n",
    "        print(\"\\nPlotting multiple metrics by nationality...\")\n",
    "        fig4 = plot.plot_group_metric_all(bias_df, \n",
    "                                metrics=['precision', 'recall', 'fpr', 'for'],\n",
    "                                attribute_name='nationality',\n",
    "                                fig_size=(14, 10))\n",
    "\n",
    "        # Fairness evaluations\n",
    "        if 'fairness_df' in locals() and isinstance(fairness_df, pd.DataFrame) and not fairness_df.empty:\n",
    "            print(\"\\nPlotting fairness results...\")\n",
    "            fig5 = plot.plot_fairness_group(fairness_df, \n",
    "                                    group_metric='precision',\n",
    "                                    title='Precision Fairness by Demographic Group',\n",
    "                                    attribute_name='nationality',\n",
    "                                    fig_size=(12, 8))\n",
    "        else:\n",
    "            print(\"\\nSkipping fairness plot due to missing fairness data\")\n",
    "    else:\n",
    "        print(\"\\nCannot use Aequitas plotting due to missing bias metrics\")\n",
    "        \n",
    "        # Create custom disparity plots using the simple disparity calculations\n",
    "        if 'simple_disparities' in locals() and isinstance(simple_disparities, dict):\n",
    "            print(\"\\nCreating custom disparity plots...\")\n",
    "            \n",
    "            # Plot nationality disparities\n",
    "            if 'nationality' in simple_disparities:\n",
    "                fig1 = create_custom_disparity_plot(\n",
    "                    simple_disparities['nationality'], \n",
    "                    'nationality', \n",
    "                    'moroccan',\n",
    "                    'Score Disparity by Nationality (reference: moroccan)'\n",
    "                )\n",
    "                plt.show()\n",
    "            \n",
    "            # Plot gender disparities\n",
    "            if 'gender' in simple_disparities:\n",
    "                fig2 = create_custom_disparity_plot(\n",
    "                    simple_disparities['gender'], \n",
    "                    'gender', \n",
    "                    'male',\n",
    "                    'Score Disparity by Gender (reference: male)'\n",
    "                )\n",
    "                plt.show()\n",
    "                \n",
    "            # Plot age group disparities\n",
    "            if 'age_group' in simple_disparities:\n",
    "                fig3 = create_custom_disparity_plot(\n",
    "                    simple_disparities['age_group'], \n",
    "                    'age_group', \n",
    "                    '25-34',\n",
    "                    'Score Disparity by Age Group (reference: 25-34)'\n",
    "                )\n",
    "                plt.show()\n",
    "                \n",
    "            # Create a heatmap of average scores by demographic intersections\n",
    "            print(\"\\nCreating heatmap of scores by demographic intersections...\")\n",
    "            \n",
    "            # Create pivot tables for intersectional analysis\n",
    "            gender_nationality_pivot = pd.pivot_table(\n",
    "                bias_audit_df, \n",
    "                values='score', \n",
    "                index=['gender'], \n",
    "                columns=['nationality'],\n",
    "                aggfunc='mean'\n",
    "            )\n",
    "            \n",
    "            age_nationality_pivot = pd.pivot_table(\n",
    "                bias_audit_df, \n",
    "                values='score', \n",
    "                index=['age_group'], \n",
    "                columns=['nationality'],\n",
    "                aggfunc='mean'\n",
    "            )\n",
    "            \n",
    "            # Plot heatmaps\n",
    "            plt.figure(figsize=(14, 6))\n",
    "            sns.heatmap(gender_nationality_pivot, annot=True, cmap='viridis', fmt='.2f', cbar_kws={'label': 'Average Score'})\n",
    "            plt.title('Average Scores by Gender and Nationality Intersection', fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure(figsize=(14, 6))\n",
    "            sns.heatmap(age_nationality_pivot, annot=True, cmap='viridis', fmt='.2f', cbar_kws={'label': 'Average Score'})\n",
    "            plt.title('Average Scores by Age Group and Nationality Intersection', fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No disparity data available for plotting\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"\\nError in plotting: {e}\")\n",
    "    print(\"Continuing with the rest of the analysis...\")\n",
    "    \n",
    "    # Create basic visualizations if Aequitas plotting fails\n",
    "    print(\"\\nCreating basic visualizations for demographic disparities...\")\n",
    "    \n",
    "    # Plot average scores by demographic group\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 18))\n",
    "    \n",
    "    # Nationality\n",
    "    nationality_scores = bias_audit_df.groupby('nationality')['score'].mean().sort_values(ascending=False)\n",
    "    nationality_counts = bias_audit_df.groupby('nationality').size()\n",
    "    \n",
    "    ax0 = sns.barplot(x=nationality_scores.index, y=nationality_scores.values, ax=axes[0])\n",
    "    axes[0].set_title('Average Scores by Nationality', fontsize=14)\n",
    "    axes[0].set_ylabel('Average Score', fontsize=12)\n",
    "    axes[0].set_xlabel('Nationality', fontsize=12)\n",
    "    axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, v in enumerate(nationality_scores.values):\n",
    "        ax0.text(i, v + 0.02, f\"n={nationality_counts[nationality_scores.index[i]]}\", ha='center')\n",
    "    \n",
    "    # Gender\n",
    "    gender_scores = bias_audit_df.groupby('gender')['score'].mean().sort_values(ascending=False)\n",
    "    gender_counts = bias_audit_df.groupby('gender').size()\n",
    "    \n",
    "    ax1 = sns.barplot(x=gender_scores.index, y=gender_scores.values, ax=axes[1])\n",
    "    axes[1].set_title('Average Scores by Gender', fontsize=14)\n",
    "    axes[1].set_ylabel('Average Score', fontsize=12)\n",
    "    axes[1].set_xlabel('Gender', fontsize=12)\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, v in enumerate(gender_scores.values):\n",
    "        ax1.text(i, v + 0.02, f\"n={gender_counts[gender_scores.index[i]]}\", ha='center')\n",
    "    \n",
    "    # Age Group\n",
    "    age_scores = bias_audit_df.groupby('age_group')['score'].mean().sort_values(ascending=False)\n",
    "    age_counts = bias_audit_df.groupby('age_group').size()\n",
    "    \n",
    "    ax2 = sns.barplot(x=age_scores.index, y=age_scores.values, ax=axes[2])\n",
    "    axes[2].set_title('Average Scores by Age Group', fontsize=14)\n",
    "    axes[2].set_ylabel('Average Score', fontsize=12)\n",
    "    axes[2].set_xlabel('Age Group', fontsize=12)\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, v in enumerate(age_scores.values):\n",
    "        ax2.text(i, v + 0.02, f\"n={age_counts[age_scores.index[i]]}\", ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a0985",
   "metadata": {},
   "source": [
    "## 6. Detailed Analysis of Demographic Impact\n",
    "\n",
    "Let's analyze how the RAG system performs across different demographic intersections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658d157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze intersectional biases (combinations of attributes)\n",
    "def analyze_intersectional_bias(results_df, attributes, score_col='relevance_score'):\n",
    "    \"\"\"Analyze bias across combinations of demographic attributes\"\"\"\n",
    "    \n",
    "    # Create a combined attribute column\n",
    "    results_df['demographic_intersection'] = results_df[attributes].apply(\n",
    "        lambda x: ' + '.join(x.values.astype(str)), axis=1)\n",
    "    \n",
    "    # Calculate mean scores by intersection\n",
    "    intersection_scores = results_df.groupby('demographic_intersection')[score_col].agg(\n",
    "        ['mean', 'count']).sort_values(by='mean', ascending=False)\n",
    "    \n",
    "    # Only keep intersections with enough data\n",
    "    intersection_scores = intersection_scores[intersection_scores['count'] >= 2]\n",
    "    \n",
    "    return intersection_scores\n",
    "\n",
    "# Analyze intersectional bias\n",
    "print(\"Analyzing intersectional bias between gender and nationality...\")\n",
    "gender_nationality = analyze_intersectional_bias(results_df, ['gender', 'nationality'])\n",
    "print(gender_nationality)\n",
    "\n",
    "print(\"\\nAnalyzing intersectional bias between age group and nationality...\")\n",
    "age_nationality = analyze_intersectional_bias(results_df, ['age_group', 'nationality'])\n",
    "print(age_nationality)\n",
    "\n",
    "# Visualize the top and bottom intersections\n",
    "def plot_top_bottom_intersections(intersection_scores, title, n=5):\n",
    "    \"\"\"Plot the top and bottom performing demographic intersections\"\"\"\n",
    "    \n",
    "    # Get top and bottom groups\n",
    "    top_n = intersection_scores.head(n)\n",
    "    bottom_n = intersection_scores.tail(n)\n",
    "    \n",
    "    # Combine for plotting\n",
    "    plot_data = pd.concat([top_n, bottom_n])\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Create colormap\n",
    "    colors = np.where(plot_data.index.isin(top_n.index), 'green', 'red')\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = plt.bar(plot_data.index, plot_data['mean'], color=colors)\n",
    "    \n",
    "    # Add counts as text on bars\n",
    "    for bar, count in zip(bars, plot_data['count']):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, \n",
    "                bar.get_height() + 0.01, \n",
    "                f\"n={count}\", \n",
    "                ha='center',\n",
    "                fontweight='bold')\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Demographic Intersection', fontsize=12)\n",
    "    plt.ylabel('Mean Relevance Score', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add legend\n",
    "    import matplotlib.patches as mpatches\n",
    "    green_patch = mpatches.Patch(color='green', label='Top performing groups')\n",
    "    red_patch = mpatches.Patch(color='red', label='Bottom performing groups')\n",
    "    plt.legend(handles=[green_patch, red_patch], loc='upper right')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot top and bottom intersections\n",
    "plot_top_bottom_intersections(\n",
    "    gender_nationality, \n",
    "    'Top and Bottom Performing Gender + Nationality Intersections', \n",
    "    n=3)\n",
    "\n",
    "plot_top_bottom_intersections(\n",
    "    age_nationality, \n",
    "    'Top and Bottom Performing Age Group + Nationality Intersections',\n",
    "    n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e626443b",
   "metadata": {},
   "source": [
    "## 7. Bias Mitigation Strategies\n",
    "\n",
    "Based on our analysis, we can implement several strategies to mitigate bias in our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af5cb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bias mitigation strategies for RAG systems\n",
    "bias_mitigation_strategies = {\n",
    "    \"data\": [\n",
    "        \"Diversify training corpus to ensure equal representation across demographics\",\n",
    "        \"Augment data for underrepresented groups\",\n",
    "        \"Balance the knowledge base across different cultural contexts\",\n",
    "        \"Include content from diverse authors and sources\"\n",
    "    ],\n",
    "    \"retrieval\": [\n",
    "        \"Implement fairness-aware retrieval algorithms\",\n",
    "        \"Apply equal representation constraints during document retrieval\",\n",
    "        \"Use bias-detecting metrics to filter or re-rank retrieved documents\",\n",
    "        \"Implement demographic-aware re-ranking\"\n",
    "    ],\n",
    "    \"prompting\": [\n",
    "        \"Design culturally-neutral prompts\",\n",
    "        \"Include explicit fairness instructions in prompts\",\n",
    "        \"Use prompt templates that encourage inclusive responses\",\n",
    "        \"Add bias awareness to system prompts\"\n",
    "    ],\n",
    "    \"monitoring\": [\n",
    "        \"Implement continuous bias monitoring across demographic groups\",\n",
    "        \"Set up alerts for fairness metric degradation\",\n",
    "        \"Periodically retrain using bias-aware techniques\",\n",
    "        \"Create dashboards for demographic performance parity\"\n",
    "    ],\n",
    "    \"evaluation\": [\n",
    "        \"Evaluate system across diverse user personas\",\n",
    "        \"Include fairness metrics in evaluation frameworks\",\n",
    "        \"Conduct regular bias audits using tools like Aequitas\",\n",
    "        \"Incorporate user feedback from diverse demographic groups\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Simple function to display strategies\n",
    "def display_strategies(strategies):\n",
    "    for category, items in strategies.items():\n",
    "        print(f\"\\n## {category.capitalize()} Strategies:\")\n",
    "        for i, strategy in enumerate(items, 1):\n",
    "            print(f\"{i}. {strategy}\")\n",
    "\n",
    "# Display bias mitigation strategies\n",
    "print(\"# Bias Mitigation Strategies for RAG Systems\")\n",
    "display_strategies(bias_mitigation_strategies)\n",
    "\n",
    "# Create a simulated bias mitigation implementation plan\n",
    "print(\"\\n# Implementation Plan Example\")\n",
    "print(\"\"\"\n",
    "## Short-term Actions (1-2 weeks)\n",
    "1. Implement fairness metrics monitoring in the evaluation pipeline\n",
    "2. Add demographic tracking to user queries (with appropriate privacy controls)\n",
    "3. Create bias awareness prompts for the RAG system\n",
    "\n",
    "## Medium-term Actions (1-2 months)\n",
    "1. Augment knowledge base with content from underrepresented groups\n",
    "2. Implement demographic-aware re-ranking algorithm\n",
    "3. Develop fairness-aware retrieval mechanisms\n",
    "\n",
    "## Long-term Actions (3-6 months)\n",
    "1. Build comprehensive bias monitoring dashboard\n",
    "2. Develop automated bias detection and mitigation system\n",
    "3. Conduct regular external audits of system performance\n",
    "4. Create a diverse evaluation dataset covering all demographic groups\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40591aff",
   "metadata": {},
   "source": [
    "## 8. Create a Simple Monitoring Dashboard\n",
    "\n",
    "Let's create a simple monitoring dashboard for tracking bias metrics over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdbeecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simulated time series of bias metrics\n",
    "import datetime\n",
    "\n",
    "# Generate dates for the past 30 days\n",
    "dates = [(datetime.datetime.now() - datetime.timedelta(days=i)).strftime('%Y-%m-%d') \n",
    "         for i in range(30, 0, -1)]\n",
    "\n",
    "# Helper function to generate bias metrics time series with gradual improvement\n",
    "def generate_bias_metrics_series(start_value, end_value, dates, noise_level=0.05):\n",
    "    values = np.linspace(start_value, end_value, len(dates))\n",
    "    # Add some noise for more realistic data\n",
    "    noise = np.random.normal(0, noise_level, len(dates))\n",
    "    return [max(0, min(1, v + n)) for v, n in zip(values, noise)]\n",
    "\n",
    "# Generate bias metrics time series for different demographics\n",
    "bias_metrics_df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'moroccan_fairness': generate_bias_metrics_series(0.95, 0.98, dates),\n",
    "    'french_fairness': generate_bias_metrics_series(0.91, 0.95, dates),\n",
    "    'american_fairness': generate_bias_metrics_series(0.89, 0.94, dates),\n",
    "    'nigerian_fairness': generate_bias_metrics_series(0.75, 0.85, dates),\n",
    "    'chinese_fairness': generate_bias_metrics_series(0.72, 0.83, dates),\n",
    "    'male_fairness': generate_bias_metrics_series(0.94, 0.96, dates),\n",
    "    'female_fairness': generate_bias_metrics_series(0.82, 0.91, dates),\n",
    "    'non_binary_fairness': generate_bias_metrics_series(0.79, 0.89, dates),\n",
    "    '18_24_fairness': generate_bias_metrics_series(0.93, 0.96, dates),\n",
    "    '25_34_fairness': generate_bias_metrics_series(0.95, 0.97, dates),\n",
    "    '35_44_fairness': generate_bias_metrics_series(0.89, 0.93, dates),\n",
    "    '45plus_fairness': generate_bias_metrics_series(0.80, 0.89, dates)\n",
    "})\n",
    "\n",
    "# Convert date to datetime\n",
    "bias_metrics_df['date'] = pd.to_datetime(bias_metrics_df['date'])\n",
    "\n",
    "# Plot the time series\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot nationality fairness trends\n",
    "plt.subplot(3, 1, 1)\n",
    "for col in ['moroccan_fairness', 'french_fairness', 'american_fairness', 'nigerian_fairness', 'chinese_fairness']:\n",
    "    plt.plot(bias_metrics_df['date'], bias_metrics_df[col], marker='o', label=col.replace('_fairness', ''))\n",
    "plt.title('Nationality Fairness Trends', fontsize=14)\n",
    "plt.ylabel('Fairness Score', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0.7, 1.0)\n",
    "plt.legend()\n",
    "\n",
    "# Plot gender fairness trends\n",
    "plt.subplot(3, 1, 2)\n",
    "for col in ['male_fairness', 'female_fairness', 'non_binary_fairness']:\n",
    "    plt.plot(bias_metrics_df['date'], bias_metrics_df[col], marker='o', label=col.replace('_fairness', ''))\n",
    "plt.title('Gender Fairness Trends', fontsize=14)\n",
    "plt.ylabel('Fairness Score', fontsize=12) \n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0.7, 1.0)\n",
    "plt.legend()\n",
    "\n",
    "# Plot age group fairness trends\n",
    "plt.subplot(3, 1, 3)\n",
    "for col in ['18_24_fairness', '25_34_fairness', '35_44_fairness', '45plus_fairness']:\n",
    "    plt.plot(bias_metrics_df['date'], bias_metrics_df[col], marker='o', \n",
    "             label=col.replace('_fairness', '').replace('_', '-'))\n",
    "plt.title('Age Group Fairness Trends', fontsize=14)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Fairness Score', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0.7, 1.0)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../bias_monitoring_trends.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Bias monitoring dashboard created and saved as 'bias_monitoring_trends.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79858d9",
   "metadata": {},
   "source": [
    "## 9. Implement a Simple Bias-Aware RAG System\n",
    "\n",
    "Let's demonstrate how to modify a RAG system to be more aware of potential bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example implementation of a bias-aware RAG system\n",
    "\n",
    "def bias_aware_rag_query(question, user_attributes=None):\n",
    "    \"\"\"\n",
    "    A bias-aware RAG query function that attempts to mitigate potential bias\n",
    "    \n",
    "    Args:\n",
    "        question: The user's question\n",
    "        user_attributes: Optional demographic attributes for monitoring\n",
    "        \n",
    "    Returns:\n",
    "        Response and metadata\n",
    "    \"\"\"\n",
    "    # 1. First, create a bias-aware prompt enhancement\n",
    "    bias_aware_prompt = \"\"\"\n",
    "    Please provide an informative, accurate answer based on the retrieved content.\n",
    "    Ensure your response is fair, balanced, and avoids cultural, gender, or age biases.\n",
    "    Consider diverse perspectives and be inclusive in your language.\n",
    "    Base your answer solely on the factual content provided.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 2. Make sure retrieval considers diversity\n",
    "    def diversity_aware_retrieval(question):\n",
    "        \"\"\"\n",
    "        Enhanced retrieval function that considers diversity of sources\n",
    "        \"\"\"\n",
    "        # Simulated retrieval function\n",
    "        # In a real system, this would:\n",
    "        # - Ensure sources from diverse perspectives\n",
    "        # - Balance cultural contexts in retrieved documents\n",
    "        # - Apply fairness constraints to retrieval\n",
    "        \n",
    "        return [\n",
    "            \"First simulated retrieved content\",\n",
    "            \"Second simulated retrieved content from diverse perspective\",\n",
    "            \"Third simulated retrieved content with balanced viewpoint\"\n",
    "        ]\n",
    "    \n",
    "    # 3. Retrieve content\n",
    "    retrieved_content = diversity_aware_retrieval(question)\n",
    "    \n",
    "    # 4. Generate response (simulated)\n",
    "    response = f\"This is a bias-aware response to: {question}\"\n",
    "    \n",
    "    # 5. Log demographic information for monitoring (if provided)\n",
    "    if user_attributes:\n",
    "        # In a real system, this would anonymize and store metrics by demographic group\n",
    "        print(f\"Logging query metrics for demographic analysis: {user_attributes}\")\n",
    "    \n",
    "    # 6. Return response with metadata\n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"bias_aware\": True,\n",
    "        \"diversity_enhanced_retrieval\": True,\n",
    "        \"retrieved_content_count\": len(retrieved_content)\n",
    "    }\n",
    "\n",
    "# Demonstrate with a test question\n",
    "test_question = \"What are the housing options for students at AUI?\"\n",
    "test_attributes = {\"nationality\": \"nigerian\", \"gender\": \"female\", \"age_group\": \"18-24\"}\n",
    "\n",
    "response = bias_aware_rag_query(test_question, test_attributes)\n",
    "print(\"Bias-aware RAG response:\")\n",
    "print(json.dumps(response, indent=2))\n",
    "\n",
    "# Example of demographic-aware prompt enhancement\n",
    "def create_demographic_aware_prompt(question, demographics=None):\n",
    "    \"\"\"Create a prompt that's sensitive to demographic context\"\"\"\n",
    "    \n",
    "    base_prompt = \"\"\"\n",
    "    Answer the following question based on the retrieved context.\n",
    "    Provide a factual, informative response.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add demographic awareness when appropriate\n",
    "    if demographics:\n",
    "        # For international students\n",
    "        if demographics.get(\"nationality\") != \"moroccan\":\n",
    "            base_prompt += \"\"\"\n",
    "            Make sure to explain any Morocco-specific or AUI-specific terms that may not be familiar to international students.\n",
    "            \"\"\"\n",
    "        \n",
    "        # For older students\n",
    "        if demographics.get(\"age_group\") in [\"35-44\", \"45+\"]:\n",
    "            base_prompt += \"\"\"\n",
    "            Consider the perspective of non-traditional students who may have different needs and experiences.\n",
    "            \"\"\"\n",
    "    \n",
    "    # Always add bias mitigation instructions\n",
    "    base_prompt += \"\"\"\n",
    "    Ensure your response is culturally inclusive and avoids assumptions based on gender, nationality, or age.\n",
    "    \"\"\"\n",
    "    \n",
    "    return base_prompt + f\"\\nQuestion: {question}\"\n",
    "\n",
    "# Show examples of demographic-aware prompts\n",
    "print(\"\\nExample of demographic-aware prompts:\")\n",
    "\n",
    "print(\"\\n1. For a Moroccan student:\")\n",
    "print(create_demographic_aware_prompt(\n",
    "    \"How do I apply for campus housing?\",\n",
    "    {\"nationality\": \"moroccan\", \"gender\": \"male\", \"age_group\": \"18-24\"}\n",
    "))\n",
    "\n",
    "print(\"\\n2. For an international student:\")\n",
    "print(create_demographic_aware_prompt(\n",
    "    \"How do I apply for campus housing?\",\n",
    "    {\"nationality\": \"chinese\", \"gender\": \"female\", \"age_group\": \"18-24\"}\n",
    "))\n",
    "\n",
    "print(\"\\n3. For an older student:\")\n",
    "print(create_demographic_aware_prompt(\n",
    "    \"How do I apply for campus housing?\",\n",
    "    {\"nationality\": \"american\", \"gender\": \"male\", \"age_group\": \"45+\"}\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e5d09e",
   "metadata": {},
   "source": [
    "## 10. Conclusion and Recommendations\n",
    "\n",
    "Based on our bias audit, we can make the following recommendations for implementing a fair and unbiased RAG system for AUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac838f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations based on our findings\n",
    "\n",
    "recommendations = {\n",
    "    \"data_enhancement\": [\n",
    "        \"Expand the university corpus with materials representing diverse student demographics\",\n",
    "        \"Include content specifically for international students (in English)\",\n",
    "        \"Ensure housing, counseling, and admission information addresses needs of all age groups\",\n",
    "        \"Add more content authored by diverse faculty and staff members\"\n",
    "    ],\n",
    "    \"system_modifications\": [\n",
    "        \"Implement demographic-aware prompting to provide appropriate context\",\n",
    "        \"Add fairness constraints to the retrieval system\",\n",
    "        \"Create a bias monitoring pipeline with weekly reports\",\n",
    "        \"Set up alerting when demographic disparity exceeds threshold of 15%\"\n",
    "    ],\n",
    "    \"evaluation_practices\": [\n",
    "        \"Include user feedback from diverse demographic groups in ongoing evaluation\",\n",
    "        \"Conduct quarterly bias audits using Aequitas\",\n",
    "        \"Establish fairness benchmarks required for any system updates\",\n",
    "        \"Create a diverse test set with questions from different demographic perspectives\"\n",
    "    ],\n",
    "    \"governance\": [\n",
    "        \"Establish a fairness review board with representatives from diverse backgrounds\",\n",
    "        \"Create clear guidelines for acceptable bias thresholds\",\n",
    "        \"Implement transparent reporting of system performance across demographics\",\n",
    "        \"Develop a bias response protocol for addressing discovered issues\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display recommendations\n",
    "print(\"# Recommendations for Implementing a Fair RAG System at AUI\\n\")\n",
    "\n",
    "for category, items in recommendations.items():\n",
    "    print(f\"## {category.replace('_', ' ').title()}\\n\")\n",
    "    for item in items:\n",
    "        print(f\"- {item}\")\n",
    "    print()\n",
    "\n",
    "# Save the bias audit summary\n",
    "audit_summary = {\n",
    "    \"date\": datetime.datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    \"system_name\": \"AUIChat RAG System\",\n",
    "    \"bias_findings\": {\n",
    "        \"nationality\": \"Moderate bias detected against non-Moroccan nationalities, particularly Chinese and Nigerian users\",\n",
    "        \"gender\": \"Slight bias detected against female users\",\n",
    "        \"age_group\": \"Moderate bias detected against users 45+\"\n",
    "    },\n",
    "    \"recommendations\": recommendations\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "with open(\"../bias_audit_summary.json\", \"w\") as f:\n",
    "    json.dump(audit_summary, f, indent=2)\n",
    "\n",
    "print(\"Bias audit summary saved to '../bias_audit_summary.json'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

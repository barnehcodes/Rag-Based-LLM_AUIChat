{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57be8bba-aea4-4c32-a2dc-c18adbc7077f",
   "metadata": {},
   "source": [
    "# Milestone 3\n",
    "## Ingestion of raw data and storage into a repository\n",
    "## Data preprocessing and feature engineering\n",
    "## Data validation/verification\n",
    "###  Connect to Qdrant Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a0e2e30-b539-4234-9e13-eb0b35562def",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# ðŸ”¹ Qdrant connection details (modify if needed)\n",
    "QDRANT_HOST = \"https://40003a30-70d7-4886-9de5-45e25681c36e.europe-west3-0.gcp.cloud.qdrant.io\"  # Example: \"localhost\" or Qdrant cloud URL\n",
    "COLLECTION_NAME = \"AUIChatVectoreCol\"\n",
    "\n",
    "# ðŸ”¹ Connect to Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    host=\"40003a30-70d7-4886-9de5-45e25681c36e.europe-west3-0.gcp.cloud.qdrant.io\",\n",
    "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.uea3Q5G9lcLfqCwxzTpRKWcMh5XM0pvPB2RaeOaDPxM\",\n",
    "    https=True\n",
    ")\n",
    "# Check if collection exists, if not create it\n",
    "\"\"\"try:\n",
    "    collection_info = qdrant_client.get_collection(COLLECTION_NAME)\n",
    "    print(f\"Collection '{COLLECTION_NAME}' exists with {collection_info.points_count} vectors\")\n",
    "except Exception:\n",
    "    print(f\"Creating new collection '{COLLECTION_NAME}'\")\n",
    "    # Create collection with appropriate configuration\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=768,  # msmarco-distilbert-base-v4 embedding size\n",
    "            distance=models.Distance.COSINE\n",
    "        )\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "# ðŸ”¹ Store raw documents in Qdrant (instead of local storage)\n",
    "#document_store = QdrantDocumentStore(qdrant_client, collection_name=COLLECTION_NAME)\n",
    "\n",
    "# ðŸ”¹ Load msmarco-distilbert-base-v4 as embedding model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/msmarco-distilbert-base-v4\")\n",
    "\n",
    "# Initialize the Qdrant vector store with full data stored in Qdrant\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client, \n",
    "    collection_name=COLLECTION_NAME,\n",
    "    # These key settings ensure we store everything in Qdrant\n",
    "    text_key=\"text\",  # Store actual text directly in Qdrant\n",
    "    metadata_key=\"metadata\",  # Store metadata directly in Qdrant\n",
    "    content_key=\"content\",  # Store full node content in Qdrant\n",
    "    embed_dim=768,  # Must match embedding dimension\n",
    "    # Critical: Store document content directly in Qdrant\n",
    "    stores_text=True,  # Tell LlamaIndex we're storing text in Qdrant\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9d6287e-35ea-4874-b75b-5b331569e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "\n",
    "# ðŸ”¹ Load university documents from PDFs\n",
    "#documents = SimpleDirectoryReader(\"resources/\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18b963-c2b3-42af-a56b-6c1dc3cdf49e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* Cleaning and better chuncking applied before embeding to enhance teh score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f4bc070-0e55-45bf-b08d-b57de9ca1871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans and normalizes text before embedding.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[^a-zA-Z0-9,.!?;:\\'\\\"()\\[\\]\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Load and process documents\n",
    "def process_documents(directory=\"resources/\"):\n",
    "    # Load documents\n",
    "    print(f\"Loading documents from {directory}\")\n",
    "    documents = SimpleDirectoryReader(directory).load_data()\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    \n",
    "    # Apply smart chunking\n",
    "    text_splitter = SentenceSplitter(chunk_size=400, chunk_overlap=50)\n",
    "    nodes = text_splitter.get_nodes_from_documents(documents)\n",
    "    print(f\"Created {len(nodes)} chunks\")\n",
    "    \n",
    "    # Apply cleaning to chunks and set metadata\n",
    "    for node in nodes:\n",
    "        node.text = clean_text(node.text)\n",
    "        # Include full text in metadata for easy retrieval\n",
    "        node.metadata = {\n",
    "            \"file_name\": node.metadata.get(\"file_name\", \"Unknown\"),\n",
    "            \"page_label\": node.metadata.get(\"page_label\", \"Unknown\"),\n",
    "            \"text\": node.text,  # Store text directly in metadata\n",
    "            \"chunk_id\": str(node.id_)  # Add ID for reference\n",
    "        }\n",
    "    \n",
    "    # Verify chunk sizes\n",
    "    chunk_sizes = [len(node.text.split()) for node in nodes]\n",
    "    print(f\"Min Chunk Size: {min(chunk_sizes)} words\")\n",
    "    print(f\"Max Chunk Size: {max(chunk_sizes)} words\")\n",
    "    print(f\"Average Chunk Size: {sum(chunk_sizes)/len(chunk_sizes):.2f} words\")\n",
    "    \n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47e26530-b2b2-4dba-9c2c-f788abe3d403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index and store everything in Qdrant (no local storage)\n",
    "def create_and_store_index(nodes):\n",
    "    # Use storage context with just the vector store (no local persistence)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    # Create an index with all data stored in Qdrant\n",
    "    index = VectorStoreIndex(\n",
    "        nodes,\n",
    "        embed_model=embed_model,\n",
    "        storage_context=storage_context,\n",
    "        store_nodes_override=True  # Force storing node content in vector DB\n",
    "    )\n",
    "    \n",
    "    print(\"All data successfully stored in Qdrant!\")\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57885f58-7794-4ea8-8256-e4604a398d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query function to retrieve from Qdrant\n",
    "def query_qdrant(query_text, limit=5):\n",
    "    # Generate embedding for the query\n",
    "    query_vector = embed_model.get_text_embedding(query_text)\n",
    "    \n",
    "    # Run similarity search in Qdrant\n",
    "    search_results = qdrant_client.search(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query_vector=query_vector,\n",
    "        limit=limit,\n",
    "        # Request complete payload with all fields\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Found {len(search_results)} results for query: '{query_text}'\")\n",
    "    \n",
    "    # Print results\n",
    "    for i, result in enumerate(search_results):\n",
    "        print(f\"\\nResult {i+1} - Score: {result.score}\")\n",
    "        print(f\"Document ID: {result.id}\")\n",
    "        \n",
    "        # Access text - try different possible locations\n",
    "        text = None\n",
    "        \n",
    "        # Try direct text field\n",
    "        if 'text' in result.payload:\n",
    "            text = result.payload.get('text')\n",
    "            file_name = result.payload.get('metadata', {}).get('file_name', \n",
    "                      result.payload.get('file_name', 'Unknown'))\n",
    "        \n",
    "        # Try metadata.text\n",
    "        elif 'metadata' in result.payload and 'text' in result.payload['metadata']:\n",
    "            text = result.payload['metadata']['text']\n",
    "            file_name = result.payload['metadata'].get('file_name', 'Unknown')\n",
    "        \n",
    "        # Try node_content\n",
    "        elif '_node_content' in result.payload:\n",
    "            node_content = json.loads(result.payload.get('_node_content', '{}'))\n",
    "            text = node_content.get(\"text\", \"No text found\")\n",
    "            file_name = node_content.get('metadata', {}).get('file_name', 'Unknown')\n",
    "        \n",
    "        # Try content\n",
    "        elif 'content' in result.payload:\n",
    "            content = result.payload['content']\n",
    "            if isinstance(content, str):\n",
    "                try:\n",
    "                    content_data = json.loads(content)\n",
    "                    text = content_data.get('text', content)\n",
    "                except:\n",
    "                    text = content\n",
    "            else:\n",
    "                text = str(content)\n",
    "            file_name = result.payload.get('file_name', 'Unknown')\n",
    "        \n",
    "        if text:\n",
    "            print(f\"File: {file_name}\")\n",
    "            print(f\"Text: {text[:500]}...\")  # Print first 500 characters\n",
    "        else:\n",
    "            print(\"No text found in payload\")\n",
    "            print(f\"Available payload keys: {list(result.payload.keys())}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb62ae77-1e8c-4ea7-8b61-7f36532b1529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import MockLLM\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.3\", token='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'\n",
    ")\n",
    "\n",
    "# Create a query engine for more advanced querying\n",
    "def create_query_engine(query_text):\n",
    "    \"\"\"Create a query engine for proper RAG queries\"\"\"\n",
    "    from llama_index.core import Settings\n",
    "    \n",
    "    # Initialize settings for the query\n",
    "    Settings.embed_model = embed_model\n",
    "    \n",
    "    # Create vector store using the same parameters as before\n",
    "    temp_vector_store = QdrantVectorStore(\n",
    "        client=qdrant_client,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        text_key=\"text\",\n",
    "        metadata_key=\"metadata\",\n",
    "        content_key=\"content\",\n",
    "        embed_dim=768,\n",
    "        stores_text=True\n",
    "    )\n",
    "    \n",
    "    # Create an empty index with the vector store\n",
    "    index = VectorStoreIndex.from_vector_store(temp_vector_store)\n",
    "    \n",
    "    # Create a query engine\n",
    "    query_engine = index.as_query_engine(llm=llm)\n",
    "    \n",
    "    # Execute query\n",
    "    response = query_engine.query(query_text)\n",
    "    \n",
    "    print(f\"\\nRAG Response for query: '{query_text}'\")\n",
    "    print(\"-\" * 50)\n",
    "    print(response)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "004480a8-8fe2-4a5a-a2ca-698d0f7238e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vectors in 'AUIChatVectoreCol': 28\n",
      "Retrieved 5 records for inspection\n",
      "\n",
      "Record ID: 02d03289-2960-41ad-8808-db7165391a04\n",
      "Payload keys: ['file_name', 'page_label', 'text', 'chunk_id', '_node_content', '_node_type', 'document_id', 'doc_id', 'ref_doc_id']\n",
      "âœ… Vector dimensions: 768\n",
      "\n",
      "Record ID: 0658fd25-e537-4a5b-8141-abadde357a84\n",
      "Payload keys: ['file_name', 'page_label', 'text', 'chunk_id', '_node_content', '_node_type', 'document_id', 'doc_id', 'ref_doc_id']\n",
      "âœ… Vector dimensions: 768\n",
      "\n",
      "Record ID: 0986a7a4-a56e-4984-9240-d6784266cdd1\n",
      "Payload keys: ['file_name', 'page_label', 'text', 'chunk_id', '_node_content', '_node_type', 'document_id', 'doc_id', 'ref_doc_id']\n",
      "âœ… Vector dimensions: 768\n",
      "\n",
      "Record ID: 1a2de18c-f519-464f-93b9-8b02387c4f1a\n",
      "Payload keys: ['file_name', 'page_label', 'text', 'chunk_id', '_node_content', '_node_type', 'document_id', 'doc_id', 'ref_doc_id']\n",
      "âœ… Vector dimensions: 768\n",
      "\n",
      "Record ID: 3885c336-7360-4f45-94c7-ee4698c263e6\n",
      "Payload keys: ['file_name', 'page_label', 'text', 'chunk_id', '_node_content', '_node_type', 'document_id', 'doc_id', 'ref_doc_id']\n",
      "âœ… Vector dimensions: 768\n",
      "âœ… Storage validation successful!\n",
      "\n",
      "--- Running Example Direct Query ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\otman\\AppData\\Local\\Temp\\ipykernel_18364\\2150494790.py:7: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 results for query: 'What are the requirements for the PiP program?'\n",
      "\n",
      "Result 1 - Score: 0.34414944\n",
      "Document ID: 43f016c7-cdf2-42c5-b141-e08956076602\n",
      "File: PiP 24-25 Program Requirements.pdf\n",
      "Text: 1 public requirements 20242025 eligibility requirements:  applicants must be recent graduates of, and be familiar with, the american style liberal arts model.  applicants must speak english fluently. proficiency in french or arabic is encouraged and appreciated, but not necessary.  applicants must have recently graduated with an undergraduate degree within the last two academic years (fallwinter 202223 or later). please note:  aui welcomes pip applications from candidates from all nationalities ...\n",
      "--------------------------------------------------\n",
      "\n",
      "Result 2 - Score: 0.19244635\n",
      "Document ID: cb149f6b-1cbd-4502-9050-034d6eeadc33\n",
      "File: Undergraduate Admission Freshmen Non-Degree Seeking.pdf\n",
      "Text: freshmen nondegreeseeking students are not required to follow a specic sequence in courses or a specic major, however, they cannot be waived from course(s) prerequisites. students enrolled in a freshman nondegree status will register for classes on a seatavailable basis. freshmen nondegreeseeking students who apply for a degree at aui and are admitted must meet all the graduation requirements indicated the catalog in force. previous courses can be transferred if they meet the selected degree req...\n",
      "--------------------------------------------------\n",
      "\n",
      "Result 3 - Score: 0.15493478\n",
      "Document ID: 4d212cc4-2bf3-43cb-8b9b-40d9b9808ee4\n",
      "File: Undergraduate Admission Process.pdf\n",
      "Text: ensure that your documents are scanned beforehand and ready for upload. the accepted le formats may vary, so please refer to the specic instructions provided on the online form. upload your academic transcripts for the past three years, a copy of your high school diploma (if already obtained), a passportsized photo, a copy of your identication, and your letter of motivation. doublecheck all the information you have provided before submitting the form. make sure all the mandatory sections are lle...\n",
      "--------------------------------------------------\n",
      "\n",
      "Result 4 - Score: 0.1327088\n",
      "Document ID: bf09ef9d-f8e6-4602-9c02-6f50987b9ef7\n",
      "File: PiP 24-25 Program Requirements.pdf\n",
      "Text: the essay should also discuss any special interest or experience the candidate has in morocco and in the broader mena region. limit 750 words. 2. an uptodate cv. 3. a letter addressed to the president of aui, dr. amine bensaid, specifying the candidates top three internship choices ranked in order of preference. applicants should submit the letter to the executive director of the office of institutional research and effectiveness at oireaui.ma . 4. two letters of recommendation from faculty memb...\n",
      "--------------------------------------------------\n",
      "\n",
      "Result 5 - Score: 0.12688273\n",
      "Document ID: 9f1d42da-c0ff-4535-9460-3616bdad76b3\n",
      "File: Undergraduate Admission Visiting.pdf\n",
      "Text: former al akhawayn students do not automatically qualify for visiting or nondegree status. converting from nondegree seeking or visiting status to regular status requires that the candidate meets the academic requirements for entry into the program. undergraduate visiting students wishing to enroll in graduatelevel courses must obtain permission from the relevant course instructor and the dean of the school. how to apply? complete and submit an online application: apply now upload the required d...\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Running Example RAG Query Engine ---\n",
      "\n",
      "RAG Response for query: 'What are the requirements for the PiP program?'\n",
      "--------------------------------------------------\n",
      "The requirements for the PiP program include being a recent graduate of an American-style liberal arts model, speaking English fluently, having graduated with an undergraduate degree within the last two academic years (fall-winter 2022-2023 or later). Applicants should also submit a short essay, and priority will be given to non-Moroccan nationals. Proficiency in French or Arabic is encouraged but not necessary. Previous study of these languages can be useful but is not required. The application process involves filling out an application form and emailing the required documents.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def validate_qdrant_storage():\n",
    "    collection_info = qdrant_client.get_collection(COLLECTION_NAME)\n",
    "    print(f\"Total vectors in '{COLLECTION_NAME}': {collection_info.points_count}\")\n",
    "\n",
    "    search_results, _ = qdrant_client.scroll(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        limit=5,\n",
    "        with_payload=True,\n",
    "        with_vectors=True\n",
    "    )\n",
    "\n",
    "    if not search_results:\n",
    "        print(\"ðŸš¨ No records retrieved from Qdrant!\")\n",
    "        return False\n",
    "\n",
    "    print(f\"Retrieved {len(search_results)} records for inspection\")\n",
    "\n",
    "    for result in search_results:\n",
    "        print(f\"\\nRecord ID: {result.id}\")\n",
    "        print(f\"Payload keys: {list(result.payload.keys())}\")\n",
    "\n",
    "        if hasattr(result, \"vector\") and result.vector is not None:\n",
    "            print(f\"âœ… Vector dimensions: {len(result.vector)}\")\n",
    "        else:\n",
    "            print(\"ðŸš¨ WARNING: Vector is None\")\n",
    "\n",
    "    print(\"âœ… Storage validation successful!\")\n",
    "    return True  # Ensure validation correctly reports success\n",
    "\n",
    "# Run validation\n",
    "if validate_qdrant_storage():\n",
    "    print(\"\\n--- Running Example Direct Query ---\")\n",
    "    query_qdrant(\"What are the requirements for the PiP program?\")\n",
    "    \n",
    "    print(\"\\n--- Running Example RAG Query Engine ---\")\n",
    "    create_query_engine(\"What are the requirements for the PiP program?\")\n",
    "else:\n",
    "    print(\"ðŸš¨ Storage validation failed - check Qdrant configuration\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6912f158-fac4-4851-8dda-bf56b5ac48b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from resources/\n",
      "Loaded 13 documents\n",
      "Created 28 chunks\n",
      "Min Chunk Size: 58 words\n",
      "Max Chunk Size: 268 words\n",
      "Average Chunk Size: 201.14 words\n",
      "All data successfully stored in Qdrant!\n",
      "\n",
      "--- Validating Qdrant Storage ---\n",
      "Total vectors in 'AUIChatVectoreCol': 56\n",
      "Retrieved 5 records for inspection\n",
      "\n",
      "Record ID: 02d03289-2960-41ad-8808-db7165391a04\n",
      "Payload keys: ['file_name', 'page_label', 'text', 'chunk_id', '_node_content', '_node_type', 'document_id', 'doc_id', 'ref_doc_id']\n",
      "âœ… Vector dimensions: 768\n",
      "\n",
      "Record ID: 041ec998-4182-4daf-a389-0ffe25e01a81\n",
      "Payload keys: ['file_name', 'page_label', 'text', 'chunk_id', '_node_content', '_node_type', 'document_id', 'doc_id', 'ref_doc_id']\n",
      "âœ… Vector dimensions: 768\n",
      "\n",
      "Record ID: 0658fd25-e537-4a5b-8141-abadde357a84\n",
      "Payload keys: ['file_name', 'page_label', 'text', 'chunk_id', '_node_content', '_node_type', 'document_id', 'doc_id', 'ref_doc_id']\n",
      "âœ… Vector dimensions: 768\n",
      "\n",
      "Record ID: 0986a7a4-a56e-4984-9240-d6784266cdd1\n",
      "Payload keys: ['file_name', 'page_label', 'text', 'chunk_id', '_node_content', '_node_type', 'document_id', 'doc_id', 'ref_doc_id']\n",
      "âœ… Vector dimensions: 768\n",
      "\n",
      "Record ID: 09a5d927-d34e-44b8-b402-6f6789920fdb\n",
      "Payload keys: ['file_name', 'page_label', 'text', 'chunk_id', '_node_content', '_node_type', 'document_id', 'doc_id', 'ref_doc_id']\n",
      "âœ… Vector dimensions: 768\n",
      "âœ… Storage validation successful!\n",
      "\n",
      "--- Running Example Direct Query ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\otman\\AppData\\Local\\Temp\\ipykernel_18364\\2150494790.py:7: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 results for query: 'What are the requirements for the PiP program?'\n",
      "\n",
      "Result 1 - Score: 0.349983\n",
      "Document ID: 896c5b4e-950f-4c44-a330-1ac7a99815f8\n",
      "File: PiP 24-25 Program Requirements.pdf\n",
      "Text: 1 public requirements 20242025 eligibility requirements:  applicants must be recent graduates of, and be familiar with, the american style liberal arts model.  applicants must speak english fluently. proficiency in french or arabic is encouraged and appreciated, but not necessary.  applicants must have recently graduated with an undergraduate degree within the last two academic years (fallwinter 202223 or later). please note:  aui welcomes pip applications from candidates from all nationalities ...\n",
      "--------------------------------------------------\n",
      "\n",
      "Result 2 - Score: 0.34414944\n",
      "Document ID: 43f016c7-cdf2-42c5-b141-e08956076602\n",
      "File: PiP 24-25 Program Requirements.pdf\n",
      "Text: 1 public requirements 20242025 eligibility requirements:  applicants must be recent graduates of, and be familiar with, the american style liberal arts model.  applicants must speak english fluently. proficiency in french or arabic is encouraged and appreciated, but not necessary.  applicants must have recently graduated with an undergraduate degree within the last two academic years (fallwinter 202223 or later). please note:  aui welcomes pip applications from candidates from all nationalities ...\n",
      "--------------------------------------------------\n",
      "\n",
      "Result 3 - Score: 0.20670305\n",
      "Document ID: 46052102-f697-43d2-ac08-d1b5748f99cd\n",
      "File: Undergraduate Admission Freshmen Non-Degree Seeking.pdf\n",
      "Text: freshmen nondegreeseeking students are not required to follow a specic sequence in courses or a specic major, however, they cannot be waived from course(s) prerequisites. students enrolled in a freshman nondegree status will register for classes on a seatavailable basis. freshmen nondegreeseeking students who apply for a degree at aui and are admitted must meet all the graduation requirements indicated the catalog in force. previous courses can be transferred if they meet the selected degree req...\n",
      "--------------------------------------------------\n",
      "\n",
      "Result 4 - Score: 0.19244635\n",
      "Document ID: cb149f6b-1cbd-4502-9050-034d6eeadc33\n",
      "File: Undergraduate Admission Freshmen Non-Degree Seeking.pdf\n",
      "Text: freshmen nondegreeseeking students are not required to follow a specic sequence in courses or a specic major, however, they cannot be waived from course(s) prerequisites. students enrolled in a freshman nondegree status will register for classes on a seatavailable basis. freshmen nondegreeseeking students who apply for a degree at aui and are admitted must meet all the graduation requirements indicated the catalog in force. previous courses can be transferred if they meet the selected degree req...\n",
      "--------------------------------------------------\n",
      "\n",
      "Result 5 - Score: 0.15493478\n",
      "Document ID: 4d212cc4-2bf3-43cb-8b9b-40d9b9808ee4\n",
      "File: Undergraduate Admission Process.pdf\n",
      "Text: ensure that your documents are scanned beforehand and ready for upload. the accepted le formats may vary, so please refer to the specic instructions provided on the online form. upload your academic transcripts for the past three years, a copy of your high school diploma (if already obtained), a passportsized photo, a copy of your identication, and your letter of motivation. doublecheck all the information you have provided before submitting the form. make sure all the mandatory sections are lle...\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Running Example RAG Query Engine ---\n",
      "\n",
      "RAG Response for query: 'What are the requirements for the PiP program?'\n",
      "--------------------------------------------------\n",
      "The requirements for the PiP program are as follows:\n",
      "\n",
      "1. Applicants must be recent graduates of an American-style liberal arts model.\n",
      "2. Applicants must speak English fluently.\n",
      "3. Applicants must have recently graduated with an undergraduate degree within the last two academic years (fall-winter 2022-23 or later).\n",
      "4. Proficiency in French or Arabic is encouraged and appreciated, but not necessary.\n",
      "5. Applicants should fill out an application form.\n",
      "6. Applicants should email a short essay (limit 750 words) presenting their interest in the PiP program, special skills, talents, or experiences that would help them contribute to the positions and the AUI community, and any special interest or experience they have in Morocco and the broader MENA region.\n",
      "7. Priority will be given to non-Moroccan nationals, and students who have completed their undergraduate education in Morocco are not eligible to apply.\n",
      "8. Previous study of Arabic and French is useful for navigating Ifrane, but it is not required, and a lack of language experience will not hurt the applic\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Process documents\n",
    "nodes = process_documents()\n",
    "    \n",
    "    # Create and store index (everything in Qdrant)\n",
    "index = create_and_store_index(nodes)\n",
    "    \n",
    "    # Validate storage\n",
    "print(\"\\n--- Validating Qdrant Storage ---\")\n",
    "\n",
    "storage_valid = validate_qdrant_storage()\n",
    "    \n",
    "if storage_valid:\n",
    "    print(\"\\n--- Running Example Direct Query ---\")\n",
    "    query_qdrant(\"What are the requirements for the PiP program?\")\n",
    "        \n",
    "    print(\"\\n--- Running Example RAG Query Engine ---\")\n",
    "    create_query_engine(\"What are the requirements for the PiP program?\")\n",
    "else:\n",
    "    print(\"Storage validation failed - please check your Qdrant configuration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e831309-2fbf-4589-b84c-e59ea2bdfee0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### step 3: Query Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c31f71-39fe-42ea-942d-e61644775604",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"What are the requirements for the PiP program?\"\n",
    "\n",
    "# ðŸ”¹ Generate embedding for the query\n",
    "query_vector = embed_model.get_text_embedding(query_text)\n",
    "\n",
    "# ðŸ”¹ Run similarity search in Qdrant\n",
    "search_results = qdrant_client.search(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    query_vector=query_vector,\n",
    "    limit=5,  # Retrieve top 5 most relevant documents\n",
    ")\n",
    "\n",
    "# ðŸ”¹ Print results\n",
    "for result in search_results:\n",
    "    print(f\"Score: {result.score}\")\n",
    "    print(f\"Document ID: {result.id}\")\n",
    "    print(f\"File: {result.payload.get('file_name', 'Unknown')}\")\n",
    "    \n",
    "    # Extract stored text\n",
    "    node_content = json.loads(result.payload.get('_node_content', '{}'))  # Decode JSON\n",
    "    document_text = node_content.get(\"text\", \"No text found\")\n",
    "\n",
    "    print(f\"Text: {document_text[:500]}...\")  # Print first 500 characters for readability\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9286980-35a8-45bd-8507-609f4151ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_info = qdrant_client.get_collection(COLLECTION_NAME)\n",
    "print(f\"Total vectors in '{COLLECTION_NAME}': {collection_info.points_count}\")\n",
    "\n",
    "# Fetch some vectors for inspection\n",
    "search_results = qdrant_client.scroll(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    limit=5\n",
    ")[0]  # scroll returns a tuple, we want the first element\n",
    "\n",
    "\n",
    "for result in search_results:\n",
    "    print(f\"ID: {result.id}\")\n",
    "    print(f\"Payload keys: {result.payload.keys()}\")\n",
    "    # Check if _node_content is in the payload\n",
    "    if '_node_content' in result.payload:\n",
    "        node_content = json.loads(result.payload['_node_content'])\n",
    "        print(f\"Node content keys: {node_content.keys()}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"âœ… Qdrant now stores both vectors & text!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29ea1f1-9bb0-4add-90fc-efa0275d1970",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sample Chunks to Confirm Cleaning \n",
    "  * lowercase âœ…\n",
    "  * free of unnecessary symbols.âœ…\n",
    "  * No excessive spaces or newlinesâœ…\n",
    "  * Chunks should end logically (not cut off mid-sentence)âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c6d83-7be2-43c9-92d2-35774a9a162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¹ Print the first 5 cleaned chunks\n",
    "print(\"\\nðŸ” Sample of Cleaned & Chunked Text:\")\n",
    "for i, node in enumerate(nodes[:5]):  # Show first 5 chunks\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    print(node.text)  # Display chunked and cleaned text\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b725c4b1-d9d6-4283-9036-c4be03ec3dc1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5e43af-8ebc-4adb-9bf6-c4ed9dfc711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ðŸ”¹ Compute chunk statistics\n",
    "chunk_sizes = [len(node.text.split()) for node in nodes]\n",
    "df = pd.DataFrame({'Chunk Size': chunk_sizes})\n",
    "\n",
    "# ðŸ”¹ Print key statistics\n",
    "print(df.describe())  # Show min, max, mean, std deviation\n",
    "\n",
    "# ðŸ”¹ Detect anomalies (extreme values)\n",
    "threshold = 500  # Adjust based on expected range\n",
    "large_chunks = df[df[\"Chunk Size\"] > threshold]\n",
    "small_chunks = df[df[\"Chunk Size\"] < 50]  # Too small to be useful\n",
    "\n",
    "print(f\"ðŸš¨ Large Chunks (>{threshold} words): {len(large_chunks)}\")\n",
    "print(f\"ðŸš¨ Small Chunks (<50 words): {len(small_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d831e34f-6286-4577-9775-de20ce8d0a49",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ðŸš¨ Problem of accessing embedings in QDRANT =under research="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ea56b2-f3b0-42d5-9ced-e95ebcfea3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¹ Retrieve first 10 stored vectors in Qdrant\n",
    "search_results = qdrant_client.scroll(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    limit=10\n",
    ")\n",
    "\n",
    "# ðŸ”¹ Validate embedding integrity\n",
    "for result in search_results[0]:\n",
    "    embedding = result.vector  # âœ… Access embeddings correctly\n",
    "    text = result.payload.get(\"text\", \"No text found\")\n",
    "\n",
    "    if embedding is None or len(embedding) == 0:  # Ensure embedding is not empty\n",
    "        print(f\"ðŸš¨ Missing embedding for chunk: {result.id}\")\n",
    "    if len(text.split()) < 10:  # Check if chunk is too small\n",
    "        print(f\"âš ï¸ Small chunk detected: {text}\")\n",
    "\n",
    "print(\"âœ… Embeddings validation completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78431528-bd0a-4f6e-8d76-5d6280354f35",
   "metadata": {},
   "source": [
    "* checking for dups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731ad4c6-c381-4f19-b6bf-72cf34acd71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# ðŸ”¹ Count occurrences of each chunk\n",
    "chunk_texts = [node.text for node in nodes]\n",
    "duplicates = [text for text, count in Counter(chunk_texts).items() if count > 1]\n",
    "\n",
    "print(f\"ðŸš¨ Duplicate Chunks Found: {len(duplicates)}\")\n",
    "if duplicates:\n",
    "    print(duplicates[:5])  # Show first few duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f17d8b2-2bbb-48e6-bb1d-186425b19601",
   "metadata": {},
   "source": [
    "* Schema Consistency\n",
    "  * each chunk follows expected metadata format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddaf20b-975e-423a-8011-3142938901c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¹ Retrieve a few stored chunks\n",
    "search_results = qdrant_client.scroll(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "for result in search_results[0]:\n",
    "    metadata = result.payload  # Metadata dictionary\n",
    "\n",
    "    # Expected keys\n",
    "    required_keys = [\"file_name\", \"text\", \"page_label\"]\n",
    "    missing_keys = [key for key in required_keys if key not in metadata]\n",
    "\n",
    "    if missing_keys:\n",
    "        print(f\"ðŸš¨ Metadata issue in chunk {result.id}: Missing {missing_keys}\")\n",
    "\n",
    "print(\"âœ… Schema validation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0993882c-e093-4dbe-8ea4-fb3441a3af54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c965753-34e9-481f-8ad7-6729f4400d23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## data validation using llama index \n",
    "* Validate Schema & Metadata in LlamaIndex\n",
    "   * Ensures metadata keys are properly stored for each chunk.\n",
    "   * Helps prevent retrieval errors due to missing metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c870a61c-bcba-48e5-b8c0-2219f396eb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Load storage context from Qdrant\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"./storage\",  # Ensure it's the correct path\n",
    "    vector_store=vector_store,  # âœ… Load stored vectors\n",
    "    docstore=document_store  # âœ… Load stored text documents\n",
    ")\n",
    "\n",
    "# âœ… Reload the index from Qdrant\n",
    "index = load_index_from_storage(storage_context, embed_model=embed_model)\n",
    "\n",
    "print(\"âœ… Index successfully loaded from Qdrant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7ed074-25b2-46ea-bed8-cf1ec1889de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¹ Retrieve all stored nodes (documents) from Qdrant\n",
    "stored_nodes = index.docstore.get_all_nodes()\n",
    "print(f\"ðŸ” Total Nodes in Qdrant Document Store: {len(stored_nodes)}\")\n",
    "\n",
    "# âœ… Validate Schema & Metadata for Each Stored Node\n",
    "expected_metadata_keys = [\"file_name\", \"page_label\", \"text\"]\n",
    "\n",
    "for node_id, node in list(stored_nodes.items())[:5]:  # Limit to first 5 for debugging\n",
    "    print(f\"ðŸ”¹ Node ID: {node_id}\")\n",
    "    print(f\"ðŸ”¹ Metadata: {node.metadata}\")\n",
    "\n",
    "    # Check if metadata contains expected keys\n",
    "    missing_keys = [key for key in expected_metadata_keys if key not in node.metadata]\n",
    "    if missing_keys:\n",
    "        print(f\"ðŸš¨ Schema Issue: Node {node_id} is missing {missing_keys}\")\n",
    "\n",
    "    print(f\"ðŸ”¹ Chunk Text Preview: {node.text[:200]}...\")  # Show first 200 characters\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"âœ… Schema validation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b075d-41c9-49e8-b2b6-947076d47eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Validate Vectors in Qdrant (Check if vectors exist for each document)\n",
    "collection_info = qdrant_client.get_collection(COLLECTION_NAME)\n",
    "print(f\"ðŸ” Total Vectors in '{COLLECTION_NAME}': {collection_info.points_count}\")\n",
    "\n",
    "# âœ… Fetch some vectors to check if they match the stored documents\n",
    "search_results = qdrant_client.scroll(collection_name=COLLECTION_NAME, limit=5)[0]  # Get first 5 results\n",
    "\n",
    "for result in search_results:\n",
    "    print(f\"ðŸ†” Vector ID: {result.id}\")\n",
    "    print(f\"ðŸ”¹ Stored Metadata Keys: {result.payload.keys()}\")\n",
    "\n",
    "    # âœ… Ensure text content is stored correctly\n",
    "    if \"text\" in result.payload:\n",
    "        print(f\"âœ… Text Found: {result.payload['text'][:200]}...\")  # Show first 200 chars\n",
    "    else:\n",
    "        print(\"ðŸš¨ Missing Text Content in Qdrant!\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"âœ… Qdrant vector-text validation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da41a0-e8e6-46ca-8f0e-db86907202a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5607ac1-52b8-40a4-b7cb-740471305050",
   "metadata": {},
   "source": [
    "# faill switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e8ab249-5ed6-410d-9f05-66733416b9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\otman\\AppData\\Local\\Temp\\ipykernel_16436\\2153126727.py:7: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qdrant_client.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Collection reset successfully!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"from qdrant_client.http import models\n",
    "\n",
    "# âš ï¸ WARNING: This will DELETE all stored vectors in the collection\n",
    "qdrant_client.delete_collection(collection_name=COLLECTION_NAME)\n",
    "\n",
    "# âœ… Recreate the collection with the correct vector configuration\n",
    "qdrant_client.recreate_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=768,  # Adjust based on your embedding model (768 for msmarco-distilbert-base-v4)\n",
    "        distance=models.Distance.COSINE  # Use COSINE for similarity-based retrieval\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"âœ… Collection reset successfully!\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050a382f-90fc-4235-9100-bd742d72128d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c1ce9-6c3f-4f77-a2e3-6239bc02004e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ec4ee6-af11-4698-ba1e-cd733c51e39a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8179fef7-93b6-482d-a2c0-d46de529627a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama_index.llms.huggingface_api\n",
      "  Downloading llama_index_llms_huggingface_api-0.4.1-py3-none-any.whl (7.2 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama_index.llms.huggingface_api) (0.29.1)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama_index.llms.huggingface_api) (0.12.22)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.23.0->llama_index.llms.huggingface_api) (6.0.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.23.0->llama_index.llms.huggingface_api) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.23.0->llama_index.llms.huggingface_api) (24.2)\n",
      "Requirement already satisfied: requests in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.23.0->llama_index.llms.huggingface_api) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.23.0->llama_index.llms.huggingface_api) (4.12.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.23.0->llama_index.llms.huggingface_api) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.23.0->llama_index.llms.huggingface_api) (3.17.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (0.9.0)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0; python_version < \"3.10\" in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (0.2.2)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (0.6.7)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (3.11.13)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (1.6.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (2.10.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (9.0.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (3.9.1)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (1.0.8)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (2.0.38)\n",
      "Requirement already satisfied: numpy in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (1.26.4)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (1.2.0)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (1.2.18)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (1.17.2)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (3.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (10.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->llama_index.llms.huggingface_api) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->llama_index.llms.huggingface_api) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->llama_index.llms.huggingface_api) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->llama_index.llms.huggingface_api) (2025.1.31)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->llama_index.llms.huggingface_api) (0.4.6)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tiktoken>=0.3.3->llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (2024.11.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (3.26.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (25.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (1.18.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (1.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (2.4.8)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (1.5.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0; python_version < \"3.11\" in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (5.0.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (0.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (6.1.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (1.0.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (2.27.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (1.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (8.1.8)\n",
      "Requirement already satisfied: greenlet!=0.4.17; python_version < \"3.14\" and (platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\")))))) in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2; python_version < \"3.11\" in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (1.2.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama_index.llms.huggingface_api) (0.14.0)\n",
      "Installing collected packages: llama-index.llms.huggingface-api\n",
      "Successfully installed llama-index.llms.huggingface-api\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\otman\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7169dfee-711a-4af9-b611-ff5817a76ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

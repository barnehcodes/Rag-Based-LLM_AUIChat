{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "import re\n",
    "import mlflow\n",
    "import time\n",
    "import numpy as np\n",
    "from sentence_transformers import util as st_util\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/barneh/Rag-Based-LLM_AUIChat/rag_based_llm_auichat/notebooks/mlruns/180508001129329925', creation_time=1742769210427, experiment_id='180508001129329925', last_update_time=1742769210427, lifecycle_stage='active', name='AUIChat-Embedding-Experiments', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"file:./mlruns\")  # local directory\n",
    "mlflow.set_experiment(\"AUIChat-Embedding-Experiments\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2Ô∏è‚É£ Utility Functions \n",
    "Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[^a-zA-Z0-9,.!?;:\\'\\\"()\\[\\]\\s]', '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3Ô∏è‚É£ Define Embedding Models to Compare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_models = {\n",
    "    \"msmarco\": \"sentence-transformers/msmarco-distilbert-base-v4\",\n",
    "    \"miniLM\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"bge\": \"BAAI/bge-small-en-v1.5\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4Ô∏è‚É£ Define Chunking Strategies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_configs = [\n",
    "    {\"chunk_size\": 400, \"overlap\": 50},\n",
    "    {\"chunk_size\": 250, \"overlap\": 25},\n",
    "    {\"chunk_size\": 100, \"overlap\": 0}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5Ô∏è‚É£ Load & Preprocess Documents üìÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"/home/barneh/Rag-Based-LLM_AUIChat/raw\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6Ô∏è‚É£ Helper Function for Running Experiments üß™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relevance_score(query, source_nodes):\n",
    "    \"\"\"Calculate relevance score between query and retrieved chunks\"\"\"\n",
    "    if not source_nodes:\n",
    "        return 0.0\n",
    "    \n",
    "    # Get the query embedding using the current embed model\n",
    "    query_embedding = Settings.embed_model.get_query_embedding(query)\n",
    "    \n",
    "    # Get embeddings for all source nodes\n",
    "    relevance_scores = []\n",
    "    for node in source_nodes:\n",
    "        if hasattr(node, 'embedding') and node.embedding is not None:\n",
    "            node_embedding = node.embedding\n",
    "        else:\n",
    "            # If node doesn't have embedding, compute it\n",
    "            node_embedding = Settings.embed_model.get_text_embedding(node.get_text())\n",
    "            \n",
    "        # Calculate cosine similarity\n",
    "        if isinstance(query_embedding, list) and isinstance(node_embedding, list):\n",
    "            query_tensor = torch.tensor([query_embedding])\n",
    "            node_tensor = torch.tensor([node_embedding])\n",
    "            score = st_util.pytorch_cos_sim(query_tensor, node_tensor).item()\n",
    "        else:\n",
    "            score = np.dot(query_embedding, node_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(node_embedding))\n",
    "            \n",
    "        relevance_scores.append(score)\n",
    "    \n",
    "    # Return average relevance score\n",
    "    return sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0.0\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Approximate token count using a simple whitespace-based approach\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "def run_experiment(model_name, model_path, chunk_size, overlap, documents):\n",
    "    \"\"\"Run a single experiment with specific model and chunk configuration\"\"\"\n",
    "    print(f\"\\nüîç Testing with model: {model_name}\")\n",
    "    print(f\"üìè Chunk size: {chunk_size}, Overlap: {overlap}\")\n",
    "    \n",
    "    # Set up embedding model\n",
    "    embed_model = HuggingFaceEmbedding(model_name=model_path)\n",
    "    Settings.embed_model = embed_model\n",
    "    \n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name=f\"{model_name}_{chunk_size}\"):\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"embedding_model\", model_name)\n",
    "        mlflow.log_param(\"chunk_size\", chunk_size)\n",
    "        mlflow.log_param(\"overlap\", overlap)\n",
    "        \n",
    "        # Process documents\n",
    "        start_time = time.time()\n",
    "        \n",
    "        splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "        nodes = splitter.get_nodes_from_documents(documents)\n",
    "        \n",
    "        for node in nodes:\n",
    "            node.text = clean_text(node.text)\n",
    "            \n",
    "        # Create index\n",
    "        storage_context = StorageContext.from_defaults()\n",
    "        index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
    "        \n",
    "        # Log metrics about the index\n",
    "        index_build_time = time.time() - start_time\n",
    "        mlflow.log_metric(\"index_build_time_seconds\", index_build_time)\n",
    "        mlflow.log_metric(\"total_chunks\", len(nodes))\n",
    "        \n",
    "        # Set up query engine\n",
    "        query_engine = index.as_query_engine(\n",
    "            llm=HuggingFaceInferenceAPI(\n",
    "                model_name=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "                token=\"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\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Run query with timing\n",
    "        query = \"What are the requirements for the PiP program?\"\n",
    "        query_start_time = time.time()\n",
    "        response = query_engine.query(query)\n",
    "        query_time = time.time() - query_start_time\n",
    "        \n",
    "        # Extract metrics from response\n",
    "        source_nodes = getattr(response, 'source_nodes', [])\n",
    "        retrieved_chunks = len(source_nodes) if source_nodes else 0\n",
    "        response_text = str(response)\n",
    "        response_tokens = count_tokens(response_text)\n",
    "        \n",
    "        # Calculate relevance score\n",
    "        relevance_score = calculate_relevance_score(query, source_nodes)\n",
    "        \n",
    "        # Log response metrics\n",
    "        mlflow.log_metric(\"response_time_seconds\", query_time)\n",
    "        mlflow.log_metric(\"retrieved_chunks\", retrieved_chunks)\n",
    "        mlflow.log_metric(\"token_count\", response_tokens)\n",
    "        mlflow.log_metric(\"relevance_score\", relevance_score)\n",
    "        mlflow.log_metric(\"tokens_per_second\", response_tokens / query_time if query_time > 0 else 0)\n",
    "        \n",
    "        # Log response text\n",
    "        mlflow.log_text(str(response), \"response.txt\")\n",
    "        \n",
    "        # Log source nodes for analysis\n",
    "        if source_nodes:\n",
    "            source_texts = \"\\n\\n---\\n\\n\".join([node.get_text() for node in source_nodes])\n",
    "            mlflow.log_text(source_texts, \"source_chunks.txt\")\n",
    "        \n",
    "        print(\"üß† Response:\")\n",
    "        print(response)\n",
    "        print(f\"‚è±Ô∏è Response Time: {query_time:.2f} seconds\")\n",
    "        print(f\"üìä Relevance Score: {relevance_score:.4f}\")\n",
    "        print(f\"üß© Chunks Retrieved: {retrieved_chunks}\")\n",
    "        print(f\"üî§ Token Count: {response_tokens}\")\n",
    "        \n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7Ô∏è‚É£ Run Individual Experiments üß™\n",
    "\n",
    "Each experiment is separated into its own cell to prevent VS Code from crashing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: msmarco with chunk size: 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing with model: msmarco\n",
      "üìè Chunk size: 400, Overlap: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6cc782b0bc54d499e5666c97454f1f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef2fe7592414fdd9896899ffd433a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60eddd4613244fa2be8cc728ff8a806d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433f8d9288d94f8ab92980b67d7db2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa87b0543df4463382d9ce4b8936ccf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/545 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127af265cc57497588c2089ad550b0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/sentence-transformers/msmarco-distilbert-base-v4/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446c3ea198164498b2767d2eeb30fd2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  67%|######7   | 178M/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/sentence-transformers/msmarco-distilbert-base-v4/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1061e018614a24a9278a013ba26390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  67%|######7   | 178M/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/sentence-transformers/msmarco-distilbert-base-v4/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de96726c86f54730a68b60edd704bc6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  91%|######### | 241M/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03acd1894c9a4d288ded933086ca62ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/319 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61dc97579974300805189be021fd1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17045c9b51774f53bce1ed513dad7108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7ad7f7a1cd4058a805747d279283f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ed9dabbd1048f1b9baa054769b0b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Response:\n",
      "The PiP program requires applicants to be recent graduates of an American-style liberal arts model. They must speak English fluently, and proficiency in French or Arabic is encouraged but not necessary. Applicants must have recently graduated with an undergraduate degree within the last two academic years. The application process involves filling out an application form and submitting a short essay, a limit of 750 words, discussing the candidate's interest in the AUI internship program, special skills, talents, or experiences, and any special interest or experience in Morocco and the broader MENA region. The application deadline is May 5th, 2024, at 23:59 GMT+1. The committee will use the cover letters and other application materials to match qualified applicants with the office best suited for their interests, experiences, and skills. Interviews will be conducted via Microsoft Teams, and results will be communicated by the week of May 20th, 2024, at the latest. For more information, contact Dr. Cherif Bel Fekih or Mrs. Sanaa Mokaddem.\n",
      "‚è±Ô∏è Response Time: 15.05 seconds\n",
      "üìä Relevance Score: 0.2529\n",
      "üß© Chunks Retrieved: 2\n",
      "üî§ Token Count: 162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response=\"The PiP program requires applicants to be recent graduates of an American-style liberal arts model. They must speak English fluently, and proficiency in French or Arabic is encouraged but not necessary. Applicants must have recently graduated with an undergraduate degree within the last two academic years. The application process involves filling out an application form and submitting a short essay, a limit of 750 words, discussing the candidate's interest in the AUI internship program, special skills, talents, or experiences, and any special interest or experience in Morocco and the broader MENA region. The application deadline is May 5th, 2024, at 23:59 GMT+1. The committee will use the cover letters and other application materials to match qualified applicants with the office best suited for their interests, experiences, and skills. Interviews will be conducted via Microsoft Teams, and results will be communicated by the week of May 20th, 2024, at the latest. For more information, contact Dr. Cherif Bel Fekih or Mrs. Sanaa Mokaddem.\", source_nodes=[NodeWithScore(node=TextNode(id_='debc7905-b510-481d-87f8-7913040949fd', embedding=None, metadata={'page_label': '1', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d8a952d1-20a5-41a9-a920-64f3ff562360', node_type='4', metadata={'page_label': '1', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, hash='51b11c90d7264658f8a0c258c793d957a7ed4d5e77c44cdc8935136fbcdcb205'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='411d9fdc-d871-4ee5-bc53-9a6b133cdd30', node_type='1', metadata={}, hash='00855e90c482b47402a0e0eee2aa7b610665c00ca7659f6821643672259cd26e')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"1 public requirements 20242025 eligibility requirements:  applicants must be recent graduates of, and be familiar with, the american style liberal arts model.  applicants must speak english fluently. proficiency in french or arabic is encouraged and appreciated, but not necessary.  applicants must have recently graduated with an undergraduate degree within the last two academic years (fallwinter 202223 or later). please note:  aui welcomes pip applications from candidates from all nationalities and citizenship statuses who have usstyle, liberal arts undergraduate degrees. however, priority will be given to nonmoroccan nationals. please note that students who have completed the entirety of their undergraduate educations in morocco are not eligible to apply.  previous interns have arrived in ifrane with vastly different levels of arabic and french knowledge, from native level fluency to no experience whatsoever. though previous study of arabic and french is useful for navigating ifrane, it is not required, and a lack of language experience will not hurt the applicants chances. the application: 1. applicants should fill out an application form. 2. applicants should email the documents below with their application form. 1. a short essay presenting the applicant's interest in the aui internship program and describing any special skills, talents, or experiences that would help the candidate contribute to the positions and to the aui community. the essay should also discuss any special interest or experience the candidate has in morocco and in the broader mena region. limit 750 words. 2.\", mimetype='text/plain', start_char_idx=0, end_char_idx=1661, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.5983887549872211), NodeWithScore(node=TextNode(id_='294f2571-bf59-4717-a657-debd9e6bdac7', embedding=None, metadata={'page_label': '2', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c4d5d68f-a9d9-4b68-b945-cc594b02364c', node_type='4', metadata={'page_label': '2', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, hash='c45622d3eab1ba1edb42f3b8e82833059464d8022f521d715ea3f6656b94a4da')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='2 public be submitted by the application deadline on may 5th, 2024 23:59 gmt1, at the latest. the committee in charge of the applications will use the cover letters and other application materials to match qualified applicants with the office best suited for their interests, experiences, and skills. each office will then interview short listed candidates via microsoft teams before reconvening and deciding on finalists. results are communicated by the week of may 20th, 2024 at the latest. acceptance aui will attempt to notify short listed candidates around early to mid may to schedule a teams interview. potential candidates will be interviewed by the supervisor of the appropriate internship, a current intern, and the internship program director. final decisions will be made soon thereafter. if a primary candidate declines an offer, an invitation may be extended to an alternate, at a later date. contact information dr. cherif bel fekih director, presidential internship program executive director, office of institutional research and effectiveness tel: 212 05 35 86 21 25 email : oireaui.ma mrs. sanaa mokaddeme manager of the office of the president tel: 212 05 35 86 20 08 email: presidentaui.ma', mimetype='text/plain', start_char_idx=0, end_char_idx=1264, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.4690018690931859)], metadata={'debc7905-b510-481d-87f8-7913040949fd': {'page_label': '1', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, '294f2571-bf59-4717-a657-debd9e6bdac7': {'page_label': '2', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run msmarco with chunk size 400\n",
    "model_name = \"msmarco\"\n",
    "model_path = embedding_models[model_name]\n",
    "cfg = chunk_configs[0]  # {\"chunk_size\": 400, \"overlap\": 50}\n",
    "run_experiment(model_name, model_path, cfg[\"chunk_size\"], cfg[\"overlap\"], documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: msmarco with chunk size: 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing with model: msmarco\n",
      "üìè Chunk size: 250, Overlap: 25\n",
      "üß† Response:\n",
      "The PiP program requires applicants to be recent graduates of the American style liberal arts model, speak English fluently, and have recently graduated with an undergraduate degree within the last two academic years. Proficiency in French or Arabic is encouraged but not necessary. Applicants should also submit an application form, a short essay, an updated CV, and a letter specifying their top three internship choices ranked in order of preference.\n",
      "‚è±Ô∏è Response Time: 6.39 seconds\n",
      "üìä Relevance Score: 0.2888\n",
      "üß© Chunks Retrieved: 2\n",
      "üî§ Token Count: 70\n",
      "üß† Response:\n",
      "The PiP program requires applicants to be recent graduates of the American style liberal arts model, speak English fluently, and have recently graduated with an undergraduate degree within the last two academic years. Proficiency in French or Arabic is encouraged but not necessary. Applicants should also submit an application form, a short essay, an updated CV, and a letter specifying their top three internship choices ranked in order of preference.\n",
      "‚è±Ô∏è Response Time: 6.39 seconds\n",
      "üìä Relevance Score: 0.2888\n",
      "üß© Chunks Retrieved: 2\n",
      "üî§ Token Count: 70\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response='The PiP program requires applicants to be recent graduates of the American style liberal arts model, speak English fluently, and have recently graduated with an undergraduate degree within the last two academic years. Proficiency in French or Arabic is encouraged but not necessary. Applicants should also submit an application form, a short essay, an updated CV, and a letter specifying their top three internship choices ranked in order of preference.', source_nodes=[NodeWithScore(node=TextNode(id_='d4b27ef9-917c-4aa4-985f-9d3ccde87e51', embedding=None, metadata={'page_label': '1', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d8a952d1-20a5-41a9-a920-64f3ff562360', node_type='4', metadata={'page_label': '1', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, hash='51b11c90d7264658f8a0c258c793d957a7ed4d5e77c44cdc8935136fbcdcb205'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d6572f43-6572-47cb-990d-a2f24fc880fd', node_type='1', metadata={}, hash='f26bdf135b44dc52ceeb0946ec751a9ded51a7e6a55c5fd57cb31229bc2ebdc4')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='1 public requirements 20242025 eligibility requirements:  applicants must be recent graduates of, and be familiar with, the american style liberal arts model.  applicants must speak english fluently. proficiency in french or arabic is encouraged and appreciated, but not necessary.  applicants must have recently graduated with an undergraduate degree within the last two academic years (fallwinter 202223 or later). please note:  aui welcomes pip applications from candidates from all nationalities and citizenship statuses who have usstyle, liberal arts undergraduate degrees. however, priority will be given to nonmoroccan nationals. please note that students who have completed the entirety of their undergraduate educations in morocco are not eligible to apply.  previous interns have arrived in ifrane with vastly different levels of arabic and french knowledge, from native level fluency to no experience whatsoever.', mimetype='text/plain', start_char_idx=0, end_char_idx=961, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6740210489629626), NodeWithScore(node=TextNode(id_='d6572f43-6572-47cb-990d-a2f24fc880fd', embedding=None, metadata={'page_label': '1', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d8a952d1-20a5-41a9-a920-64f3ff562360', node_type='4', metadata={'page_label': '1', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, hash='51b11c90d7264658f8a0c258c793d957a7ed4d5e77c44cdc8935136fbcdcb205'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='d4b27ef9-917c-4aa4-985f-9d3ccde87e51', node_type='1', metadata={'page_label': '1', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, hash='2060cc93164701bfbc26183f6b027cd10bcf2a2f840516fe867bff89d7b393ac'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c102c9ff-55db-4949-b58a-d49f96709420', node_type='1', metadata={}, hash='0423969759fd796b8f1b517e75581ae90feedfa57fdb849070ffea4278d3caac')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"though previous study of arabic and french is useful for navigating ifrane, it is not required, and a lack of language experience will not hurt the applicants chances. the application: 1. applicants should fill out an application form. 2. applicants should email the documents below with their application form. 1. a short essay presenting the applicant's interest in the aui internship program and describing any special skills, talents, or experiences that would help the candidate contribute to the positions and to the aui community. the essay should also discuss any special interest or experience the candidate has in morocco and in the broader mena region. limit 750 words. 2. an uptodate cv. 3. a letter addressed to the president of aui, dr. amine bensaid, specifying the candidates top three internship choices ranked in order of preference.\", mimetype='text/plain', start_char_idx=962, end_char_idx=1834, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6104874039941742)], metadata={'d4b27ef9-917c-4aa4-985f-9d3ccde87e51': {'page_label': '1', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, 'd6572f43-6572-47cb-990d-a2f24fc880fd': {'page_label': '1', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run msmarco with chunk size 250\n",
    "model_name = \"msmarco\"\n",
    "model_path = embedding_models[model_name]\n",
    "cfg = chunk_configs[1]  # {\"chunk_size\": 250, \"overlap\": 25}\n",
    "run_experiment(model_name, model_path, cfg[\"chunk_size\"], cfg[\"overlap\"], documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: msmarco with chunk size: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing with model: msmarco\n",
      "üìè Chunk size: 100, Overlap: 0\n",
      "üß† Response:\n",
      "The PiP program requires applicants to submit a cover letter, resume, and two letters of recommendation from faculty members, administrators, or work supervisors. The recommendation letters should highlight the student's general qualifications, Middle Eastern or African experience or interest, and their ability to serve in the specific internships. The committee will use these materials to match qualified applicants with the office that best suits their interests, experiences, and skills. After shortlisting candidates, interviews will be conducted via Microsoft Teams, and the final decisions will be made based on these interviews.\n",
      "‚è±Ô∏è Response Time: 7.78 seconds\n",
      "üìä Relevance Score: 0.1334\n",
      "üß© Chunks Retrieved: 2\n",
      "üî§ Token Count: 90\n",
      "üß† Response:\n",
      "The PiP program requires applicants to submit a cover letter, resume, and two letters of recommendation from faculty members, administrators, or work supervisors. The recommendation letters should highlight the student's general qualifications, Middle Eastern or African experience or interest, and their ability to serve in the specific internships. The committee will use these materials to match qualified applicants with the office that best suits their interests, experiences, and skills. After shortlisting candidates, interviews will be conducted via Microsoft Teams, and the final decisions will be made based on these interviews.\n",
      "‚è±Ô∏è Response Time: 7.78 seconds\n",
      "üìä Relevance Score: 0.1334\n",
      "üß© Chunks Retrieved: 2\n",
      "üî§ Token Count: 90\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response=\"The PiP program requires applicants to submit a cover letter, resume, and two letters of recommendation from faculty members, administrators, or work supervisors. The recommendation letters should highlight the student's general qualifications, Middle Eastern or African experience or interest, and their ability to serve in the specific internships. The committee will use these materials to match qualified applicants with the office that best suits their interests, experiences, and skills. After shortlisting candidates, interviews will be conducted via Microsoft Teams, and the final decisions will be made based on these interviews.\", source_nodes=[NodeWithScore(node=TextNode(id_='18b5dc8c-55c2-4191-88f2-1dbb30b24015', embedding=None, metadata={'page_label': '1', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d8a952d1-20a5-41a9-a920-64f3ff562360', node_type='4', metadata={'page_label': '1', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, hash='51b11c90d7264658f8a0c258c793d957a7ed4d5e77c44cdc8935136fbcdcb205'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='eada0929-41a7-436c-b519-2537fba5806e', node_type='1', metadata={'page_label': '1', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, hash='e6785ad3d0a4128c940c403795595644ec7c23538efbef9e60cfc2a67b0449a2'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='040a421a-a4a6-48f7-a207-fe2fa0cacf70', node_type='1', metadata={}, hash='de2e2efcf85f329795872a918017beccece0c16d24252e40648d1406446013bc')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"two letters of recommendation from faculty members, administrators, or work supervisors. the student's general qualifications, middle eastern or african experience or interest, and ability to serve in the specific internships should be identified in the recommendation.\", mimetype='text/plain', start_char_idx=1978, end_char_idx=2251, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6929917549007705), NodeWithScore(node=TextNode(id_='aeeb4a3a-776f-493d-a4ec-c4ad6ab37f9d', embedding=None, metadata={'page_label': '2', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c4d5d68f-a9d9-4b68-b945-cc594b02364c', node_type='4', metadata={'page_label': '2', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, hash='c45622d3eab1ba1edb42f3b8e82833059464d8022f521d715ea3f6656b94a4da'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8abb13ac-0449-4a38-8895-f7e1a4fd06e2', node_type='1', metadata={'page_label': '2', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, hash='603b9f5046fc95fdc8fb0e1156dfecd6f95c4e4b441c0af3a9274677b7602a8f'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='0749b1fd-762b-4fff-907a-ca8573005c20', node_type='1', metadata={}, hash='9182312dd846edd52fcd0e68f46ebc1f8a11c8cac1b4c857130daf3739bfffff')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='the committee in charge of the applications will use the cover letters and other application materials to match qualified applicants with the office best suited for their interests, experiences, and skills. each office will then interview short listed candidates via microsoft teams before reconvening and deciding on finalists.', mimetype='text/plain', start_char_idx=104, end_char_idx=437, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6765897075221402)], metadata={'18b5dc8c-55c2-4191-88f2-1dbb30b24015': {'page_label': '1', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, 'aeeb4a3a-776f-493d-a4ec-c4ad6ab37f9d': {'page_label': '2', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run msmarco with chunk size 100\n",
    "model_name = \"msmarco\"\n",
    "model_path = embedding_models[model_name]\n",
    "cfg = chunk_configs[2]  # {\"chunk_size\": 100, \"overlap\": 0}\n",
    "run_experiment(model_name, model_path, cfg[\"chunk_size\"], cfg[\"overlap\"], documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: miniLM with chunk size: 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing with model: miniLM\n",
      "üìè Chunk size: 400, Overlap: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0c132f9f6a48d581171e8c0288f0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec8c5da5ced4c57979b6902185ed38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f328b74980f84719b268f2cba1f18f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c9f8c8f01945c786f2f4c2bf22994a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5f1b9f33464d419b4009549fd55472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5cf0663035d44e5879cbab2889c9036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c21f2f50bd4d0bbd048eda10d604fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c75d7e1b07477f945285659cc5ab41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0512b728704f09bc400b171ffb7554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb4b67f33184e91b69cabb23530c805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94455f3fcd04a69967abd05b0afae39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Response:\n",
      "The requirements for the PiP program include being a recent graduate, familiar with the American style liberal arts model, fluent in English, and having recently graduated with an undergraduate degree within the last two academic years. Proficiency in French or Arabic is encouraged but not necessary. The application process involves filling out an application form and submitting a short essay, limited to 750 words, discussing the candidate's interest in the AUI internship program, special skills, talents, or experiences, and any special interest or experience in Morocco and the broader MENA region. For applicants whose first language is not English, taking the TOEFL exam for English competency is required.\n",
      "‚è±Ô∏è Response Time: 9.42 seconds\n",
      "üìä Relevance Score: 0.4539\n",
      "üß© Chunks Retrieved: 2\n",
      "üî§ Token Count: 108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response=\"The requirements for the PiP program include being a recent graduate, familiar with the American style liberal arts model, fluent in English, and having recently graduated with an undergraduate degree within the last two academic years. Proficiency in French or Arabic is encouraged but not necessary. The application process involves filling out an application form and submitting a short essay, limited to 750 words, discussing the candidate's interest in the AUI internship program, special skills, talents, or experiences, and any special interest or experience in Morocco and the broader MENA region. For applicants whose first language is not English, taking the TOEFL exam for English competency is required.\", source_nodes=[NodeWithScore(node=TextNode(id_='781eab35-475d-4549-8db6-47165fcf35b7', embedding=None, metadata={'page_label': '1', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d8a952d1-20a5-41a9-a920-64f3ff562360', node_type='4', metadata={'page_label': '1', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, hash='51b11c90d7264658f8a0c258c793d957a7ed4d5e77c44cdc8935136fbcdcb205'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='b1d6edaf-0287-43ab-9b81-798e4920b741', node_type='1', metadata={}, hash='00855e90c482b47402a0e0eee2aa7b610665c00ca7659f6821643672259cd26e')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"1 public requirements 20242025 eligibility requirements:  applicants must be recent graduates of, and be familiar with, the american style liberal arts model.  applicants must speak english fluently. proficiency in french or arabic is encouraged and appreciated, but not necessary.  applicants must have recently graduated with an undergraduate degree within the last two academic years (fallwinter 202223 or later). please note:  aui welcomes pip applications from candidates from all nationalities and citizenship statuses who have usstyle, liberal arts undergraduate degrees. however, priority will be given to nonmoroccan nationals. please note that students who have completed the entirety of their undergraduate educations in morocco are not eligible to apply.  previous interns have arrived in ifrane with vastly different levels of arabic and french knowledge, from native level fluency to no experience whatsoever. though previous study of arabic and french is useful for navigating ifrane, it is not required, and a lack of language experience will not hurt the applicants chances. the application: 1. applicants should fill out an application form. 2. applicants should email the documents below with their application form. 1. a short essay presenting the applicant's interest in the aui internship program and describing any special skills, talents, or experiences that would help the candidate contribute to the positions and to the aui community. the essay should also discuss any special interest or experience the candidate has in morocco and in the broader mena region. limit 750 words. 2.\", mimetype='text/plain', start_char_idx=0, end_char_idx=1661, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6154050702606144), NodeWithScore(node=TextNode(id_='25a3e30c-6f4e-4e73-8a92-c41e8c4bcfb5', embedding=None, metadata={'page_label': '178', 'file_name': 'AUI Catalog_2023-2024_New_Version.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/AUI Catalog_2023-2024_New_Version.pdf', 'file_type': 'application/pdf', 'file_size': 8307428, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='997411f8-b341-4360-a417-e32300a35d35', node_type='4', metadata={'page_label': '178', 'file_name': 'AUI Catalog_2023-2024_New_Version.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/AUI Catalog_2023-2024_New_Version.pdf', 'file_type': 'application/pdf', 'file_size': 8307428, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-23'}, hash='56f4661d001e8bf396ac0015783c6a022f6bb060281cc6f06d263fc69df23e46'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='22905255-be0f-4f2c-a7b7-ed5af23eb408', node_type='1', metadata={'page_label': '178', 'file_name': 'AUI Catalog_2023-2024_New_Version.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/AUI Catalog_2023-2024_New_Version.pdf', 'file_type': 'application/pdf', 'file_size': 8307428, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-23'}, hash='0a3a5f2041fbc689d883450dd768cc8cdabbf8ce00d6cec1f1108f838ac6726e')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='applicants whose first language is not english must take the toefl exam for english competency. psychology major core (24 sch) course code course name psy 1301 introduction to psychology (prerequisite: fas 0210) psy 1303 clinical psychology  psychopathology (prerequisite: psy 1301) psy 2302 cognitive sciences  cognitive psychology (prerequisite: psy 1301) psy 2303 developmental psychology (prerequisite: psy 1301) psy 2304 psychology of health (prerequisite: psy 1301) psy 3302 social and organizational psychology (prerequisite: psy 1301) psy 3305 interpersonal relations and conflict (prerequisite: psy 1301) psy 3306 human sexuality and genders (prerequisite: psy 1301)', mimetype='text/plain', start_char_idx=1535, end_char_idx=2231, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.4669502558009969)], metadata={'781eab35-475d-4549-8db6-47165fcf35b7': {'page_label': '1', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, '25a3e30c-6f4e-4e73-8a92-c41e8c4bcfb5': {'page_label': '178', 'file_name': 'AUI Catalog_2023-2024_New_Version.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/AUI Catalog_2023-2024_New_Version.pdf', 'file_type': 'application/pdf', 'file_size': 8307428, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-23'}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run miniLM with chunk size 400\n",
    "model_name = \"miniLM\"\n",
    "model_path = embedding_models[model_name]\n",
    "cfg = chunk_configs[0]  # {\"chunk_size\": 400, \"overlap\": 50}\n",
    "run_experiment(model_name, model_path, cfg[\"chunk_size\"], cfg[\"overlap\"], documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: miniLM with chunk size: 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing with model: miniLM\n",
      "üìè Chunk size: 250, Overlap: 25\n",
      "üß† Response:\n",
      "The PiP program requires applicants to be recent graduates of the American style liberal arts model, speak English fluently, and have recently graduated with an undergraduate degree within the last two academic years. Proficiency in French or Arabic is encouraged but not necessary. Applicants should note that priority will be given to non-Moroccan nationals, and students who have completed their entire undergraduate education in Morocco are not eligible to apply. The contact information for the program is available, with Dr. Cherif Bel Fekih as the Director and Dr. Sanaa Mokaddeme as the Manager of the Office of the President.\n",
      "‚è±Ô∏è Response Time: 8.40 seconds\n",
      "üìä Relevance Score: 0.3219\n",
      "üß© Chunks Retrieved: 2\n",
      "üî§ Token Count: 99\n",
      "üß† Response:\n",
      "The PiP program requires applicants to be recent graduates of the American style liberal arts model, speak English fluently, and have recently graduated with an undergraduate degree within the last two academic years. Proficiency in French or Arabic is encouraged but not necessary. Applicants should note that priority will be given to non-Moroccan nationals, and students who have completed their entire undergraduate education in Morocco are not eligible to apply. The contact information for the program is available, with Dr. Cherif Bel Fekih as the Director and Dr. Sanaa Mokaddeme as the Manager of the Office of the President.\n",
      "‚è±Ô∏è Response Time: 8.40 seconds\n",
      "üìä Relevance Score: 0.3219\n",
      "üß© Chunks Retrieved: 2\n",
      "üî§ Token Count: 99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response='The PiP program requires applicants to be recent graduates of the American style liberal arts model, speak English fluently, and have recently graduated with an undergraduate degree within the last two academic years. Proficiency in French or Arabic is encouraged but not necessary. Applicants should note that priority will be given to non-Moroccan nationals, and students who have completed their entire undergraduate education in Morocco are not eligible to apply. The contact information for the program is available, with Dr. Cherif Bel Fekih as the Director and Dr. Sanaa Mokaddeme as the Manager of the Office of the President.', source_nodes=[NodeWithScore(node=TextNode(id_='0030250c-e43b-4333-9ba2-8852b46c79c7', embedding=None, metadata={'page_label': '1', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d8a952d1-20a5-41a9-a920-64f3ff562360', node_type='4', metadata={'page_label': '1', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, hash='51b11c90d7264658f8a0c258c793d957a7ed4d5e77c44cdc8935136fbcdcb205'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d96579a9-5e77-41b5-9f40-a84d4afc7882', node_type='1', metadata={}, hash='f26bdf135b44dc52ceeb0946ec751a9ded51a7e6a55c5fd57cb31229bc2ebdc4')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='1 public requirements 20242025 eligibility requirements:  applicants must be recent graduates of, and be familiar with, the american style liberal arts model.  applicants must speak english fluently. proficiency in french or arabic is encouraged and appreciated, but not necessary.  applicants must have recently graduated with an undergraduate degree within the last two academic years (fallwinter 202223 or later). please note:  aui welcomes pip applications from candidates from all nationalities and citizenship statuses who have usstyle, liberal arts undergraduate degrees. however, priority will be given to nonmoroccan nationals. please note that students who have completed the entirety of their undergraduate educations in morocco are not eligible to apply.  previous interns have arrived in ifrane with vastly different levels of arabic and french knowledge, from native level fluency to no experience whatsoever.', mimetype='text/plain', start_char_idx=0, end_char_idx=961, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.633570541218931), NodeWithScore(node=TextNode(id_='f5c365fa-1d09-48c2-9ab3-5e5224db0bbe', embedding=None, metadata={'page_label': '2', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c4d5d68f-a9d9-4b68-b945-cc594b02364c', node_type='4', metadata={'page_label': '2', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, hash='c45622d3eab1ba1edb42f3b8e82833059464d8022f521d715ea3f6656b94a4da'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='82fe844b-aa4b-4dbc-8a01-09d7dbbea374', node_type='1', metadata={'page_label': '2', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, hash='55af710a6bd8d8fd0a4a5ed5aedf08c09d863ac3ca0eba05d31f3b28909b9263')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='contact information dr. cherif bel fekih director, presidential internship program executive director, office of institutional research and effectiveness tel: 212 05 35 86 21 25 email : oireaui.ma mrs. sanaa mokaddeme manager of the office of the president tel: 212 05 35 86 20 08 email: presidentaui.ma', mimetype='text/plain', start_char_idx=940, end_char_idx=1264, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.621945660575006)], metadata={'0030250c-e43b-4333-9ba2-8852b46c79c7': {'page_label': '1', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}, 'f5c365fa-1d09-48c2-9ab3-5e5224db0bbe': {'page_label': '2', 'file_name': 'PiP 24-25 Program Requirements.pdf', 'file_path': '/home/barneh/Rag-Based-LLM_AUIChat/raw/PiP 24-25 Program Requirements.pdf', 'file_type': 'application/pdf', 'file_size': 166707, 'creation_date': '2025-03-23', 'last_modified_date': '2025-03-20'}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run miniLM with chunk size 250\n",
    "model_name = \"miniLM\"\n",
    "model_path = embedding_models[model_name]\n",
    "cfg = chunk_configs[1]  # {\"chunk_size\": 250, \"overlap\": 25}\n",
    "run_experiment(model_name, model_path, cfg[\"chunk_size\"], cfg[\"overlap\"], documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: miniLM with chunk size: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing with model: miniLM\n",
      "üìè Chunk size: 100, Overlap: 0\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.3/v1/chat/completions (Request ID: Root=1-680554f0-7ad44f590b4b6f91353bd420;2ead2a9f-adc5-4f4a-8332-6ebf23cf7459)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.3/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m model_path = embedding_models[model_name]\n\u001b[32m      4\u001b[39m cfg = chunk_configs[\u001b[32m2\u001b[39m]  \u001b[38;5;66;03m# {\"chunk_size\": 100, \"overlap\": 0}\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchunk_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverlap\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(model_name, model_path, chunk_size, overlap, documents)\u001b[39m\n\u001b[32m     78\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mWhat are the requirements for the PiP program?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     79\u001b[39m query_start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m response = \u001b[43mquery_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m query_time = time.time() - query_start_time\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Extract metrics from response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    325\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/llama_index/core/base/base_query_engine.py:52\u001b[39m, in \u001b[36mBaseQueryEngine.query\u001b[39m\u001b[34m(self, str_or_query_bundle)\u001b[39m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m     51\u001b[39m         str_or_query_bundle = QueryBundle(str_or_query_bundle)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     query_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m dispatcher.event(\n\u001b[32m     54\u001b[39m     QueryEndEvent(query=str_or_query_bundle, response=query_result)\n\u001b[32m     55\u001b[39m )\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    325\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/llama_index/core/query_engine/retriever_query_engine.py:179\u001b[39m, in \u001b[36mRetrieverQueryEngine._query\u001b[39m\u001b[34m(self, query_bundle)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.event(\n\u001b[32m    176\u001b[39m     CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n\u001b[32m    177\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[32m    178\u001b[39m     nodes = \u001b[38;5;28mself\u001b[39m.retrieve(query_bundle)\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_response_synthesizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m     query_event.on_end(payload={EventPayload.RESPONSE: response})\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    325\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/llama_index/core/response_synthesizers/base.py:241\u001b[39m, in \u001b[36mBaseSynthesizer.synthesize\u001b[39m\u001b[34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[39m\n\u001b[32m    235\u001b[39m     query = QueryBundle(query_str=query)\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._callback_manager.event(\n\u001b[32m    238\u001b[39m     CBEventType.SYNTHESIZE,\n\u001b[32m    239\u001b[39m     payload={EventPayload.QUERY_STR: query.query_str},\n\u001b[32m    240\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     response_str = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMetadataMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m     additional_source_nodes = additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m    250\u001b[39m     source_nodes = \u001b[38;5;28mlist\u001b[39m(nodes) + \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    325\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py:43\u001b[39m, in \u001b[36mCompactAndRefine.get_response\u001b[39m\u001b[34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# use prompt helper to fix compact text_chunks under the prompt limitation\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# TODO: This is a temporary fix - reason it's temporary is that\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# the refine template does not account for size of previous answer.\u001b[39;00m\n\u001b[32m     42\u001b[39m new_texts = \u001b[38;5;28mself\u001b[39m._make_compact_text_chunks(query_str, text_chunks)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprev_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprev_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    325\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/llama_index/core/response_synthesizers/refine.py:179\u001b[39m, in \u001b[36mRefine.get_response\u001b[39m\u001b[34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text_chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks:\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prev_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    177\u001b[39m         \u001b[38;5;66;03m# if this is the first chunk, and text chunk already\u001b[39;00m\n\u001b[32m    178\u001b[39m         \u001b[38;5;66;03m# is an answer, then return it\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_give_response_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m            \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kwargs\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    183\u001b[39m         \u001b[38;5;66;03m# refine response if possible\u001b[39;00m\n\u001b[32m    184\u001b[39m         response = \u001b[38;5;28mself\u001b[39m._refine_response_single(\n\u001b[32m    185\u001b[39m             prev_response, query_str, text_chunk, **response_kwargs\n\u001b[32m    186\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/llama_index/core/response_synthesizers/refine.py:241\u001b[39m, in \u001b[36mRefine._give_response_single\u001b[39m\u001b[34m(self, query_str, text_chunk, **response_kwargs)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._streaming:\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    239\u001b[39m         structured_response = cast(\n\u001b[32m    240\u001b[39m             StructuredRefineResponse,\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m             \u001b[43mprogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcontext_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcur_text_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    245\u001b[39m         )\n\u001b[32m    246\u001b[39m         query_satisfied = structured_response.query_satisfied\n\u001b[32m    247\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m query_satisfied:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    325\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/llama_index/core/response_synthesizers/refine.py:85\u001b[39m, in \u001b[36mDefaultRefineProgram.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m     83\u001b[39m         answer = answer.model_dump_json()\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_llm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m StructuredRefineResponse(answer=answer, query_satisfied=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    325\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/llama_index/core/llms/llm.py:605\u001b[39m, in \u001b[36mLLM.predict\u001b[39m\u001b[34m(self, prompt, **prompt_args)\u001b[39m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metadata.is_chat_model:\n\u001b[32m    604\u001b[39m     messages = \u001b[38;5;28mself\u001b[39m._get_messages(prompt, **prompt_args)\n\u001b[32m--> \u001b[39m\u001b[32m605\u001b[39m     chat_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    606\u001b[39m     output = chat_response.message.content \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    325\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/llama_index/llms/huggingface_api/base.py:272\u001b[39m, in \u001b[36mHuggingFaceInferenceAPI.chat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.task == \u001b[33m\"\u001b[39m\u001b[33mconversational\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    270\u001b[39m     model_kwargs = \u001b[38;5;28mself\u001b[39m._get_model_kwargs(**kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     output: ChatCompletionOutput = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sync_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_to_huggingface_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m     content = output.choices[\u001b[32m0\u001b[39m].message.content \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m     tool_calls = output.choices[\u001b[32m0\u001b[39m].message.tool_calls \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:992\u001b[39m, in \u001b[36mInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    964\u001b[39m parameters = {\n\u001b[32m    965\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: payload_model,\n\u001b[32m    966\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   (...)\u001b[39m\u001b[32m    983\u001b[39m     **(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    984\u001b[39m }\n\u001b[32m    985\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m    986\u001b[39m     inputs=messages,\n\u001b[32m    987\u001b[39m     parameters=parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    990\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m    991\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m992\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    995\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:357\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    354\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:482\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.3/v1/chat/completions (Request ID: Root=1-680554f0-7ad44f590b4b6f91353bd420;2ead2a9f-adc5-4f4a-8332-6ebf23cf7459)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
     ]
    }
   ],
   "source": [
    "# Run miniLM with chunk size 100\n",
    "model_name = \"miniLM\"\n",
    "model_path = embedding_models[model_name]\n",
    "cfg = chunk_configs[2]  # {\"chunk_size\": 100, \"overlap\": 0}\n",
    "run_experiment(model_name, model_path, cfg[\"chunk_size\"], cfg[\"overlap\"], documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: bge with chunk size: 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run bge with chunk size 400\n",
    "model_name = \"bge\"\n",
    "model_path = embedding_models[model_name]\n",
    "cfg = chunk_configs[0]  # {\"chunk_size\": 400, \"overlap\": 50}\n",
    "run_experiment(model_name, model_path, cfg[\"chunk_size\"], cfg[\"overlap\"], documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: bge with chunk size: 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run bge with chunk size 250\n",
    "model_name = \"bge\"\n",
    "model_path = embedding_models[model_name]\n",
    "cfg = chunk_configs[1]  # {\"chunk_size\": 250, \"overlap\": 25}\n",
    "run_experiment(model_name, model_path, cfg[\"chunk_size\"], cfg[\"overlap\"], documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: bge with chunk size: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run bge with chunk size 100\n",
    "model_name = \"bge\"\n",
    "model_path = embedding_models[model_name]\n",
    "cfg = chunk_configs[2]  # {\"chunk_size\": 100, \"overlap\": 0}\n",
    "run_experiment(model_name, model_path, cfg[\"chunk_size\"], cfg[\"overlap\"], documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8Ô∏è‚É£ Result Analysis\n",
    "\n",
    "Run this cell to check all experiments in MLflow UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_name, model_path in embedding_models.items():\n",
    "#     print(f\"\\nüîç Testing with model: {model_name}\")\n",
    "#     embed_model = HuggingFaceEmbedding(model_name=model_path)\n",
    "#     Settings.embed_model = embed_model\n",
    "    \n",
    "#     for cfg in chunk_configs:\n",
    "#          with mlflow.start_run(run_name=f\"{model_name}_{cfg['chunk_size']}\"):\n",
    "\n",
    "#             # log parameters\n",
    "#             mlflow.log_param(\"embedding_model\", model_name)\n",
    "#             mlflow.log_param(\"chunk_size\", cfg[\"chunk_size\"])\n",
    "#             mlflow.log_param(\"overlap\", cfg[\"overlap\"])\n",
    "\n",
    "#             print(f\"\\nüìè Chunk size: {cfg['chunk_size']}, Overlap: {cfg['overlap']}\")\n",
    "#             splitter = SentenceSplitter(chunk_size=cfg[\"chunk_size\"], chunk_overlap=cfg[\"overlap\"])\n",
    "#             nodes = splitter.get_nodes_from_documents(documents)\n",
    "        \n",
    "#             for node in nodes:\n",
    "#                 node.text = clean_text(node.text)\n",
    "\n",
    "#             storage_context = StorageContext.from_defaults()\n",
    "#             index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
    "        \n",
    "#             query_engine = index.as_query_engine(\n",
    "#                 llm=HuggingFaceInferenceAPI(\n",
    "#                     model_name=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "#                     token=\"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\"  # your token\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#             query = \"What are the requirements for the PiP program?\"\n",
    "#             response = query_engine.query(query)\n",
    "#              # log response as artifact or metric\n",
    "#             mlflow.log_text(str(response), \"response.txt\")\n",
    "\n",
    "#             print(\"üß† Response:\")\n",
    "#             print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9Ô∏è‚É£ Viewing Results in MLflow Dashboard\n",
    "\n",
    "To visualize all experiment results in the MLflow dashboard, run the command below. This will start the MLflow UI server which you can access in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-20 16:18:29 -0400] [34586] [INFO] Starting gunicorn 23.0.0\n",
      "[2025-04-20 16:18:29 -0400] [34586] [INFO] Listening at: http://127.0.0.1:5000 (34586)\n",
      "[2025-04-20 16:18:29 -0400] [34586] [INFO] Using worker: sync\n",
      "[2025-04-20 16:18:29 -0400] [34587] [INFO] Booting worker with pid: 34587\n",
      "[2025-04-20 16:18:29 -0400] [34588] [INFO] Booting worker with pid: 34588\n",
      "[2025-04-20 16:18:29 -0400] [34586] [INFO] Listening at: http://127.0.0.1:5000 (34586)\n",
      "[2025-04-20 16:18:29 -0400] [34586] [INFO] Using worker: sync\n",
      "[2025-04-20 16:18:29 -0400] [34587] [INFO] Booting worker with pid: 34587\n",
      "[2025-04-20 16:18:29 -0400] [34588] [INFO] Booting worker with pid: 34588\n",
      "[2025-04-20 16:18:29 -0400] [34589] [INFO] Booting worker with pid: 34589\n",
      "[2025-04-20 16:18:29 -0400] [34590] [INFO] Booting worker with pid: 34590\n",
      "[2025-04-20 16:18:29 -0400] [34589] [INFO] Booting worker with pid: 34589\n",
      "[2025-04-20 16:18:29 -0400] [34590] [INFO] Booting worker with pid: 34590\n",
      "[2025-04-20 16:19:31 -0400] [34586] [INFO] Handling signal: int\n",
      "^C\n",
      "\n",
      "Aborted!\n",
      "[2025-04-20 16:19:31 -0400] [34589] [INFO] Worker exiting (pid: 34589)\n",
      "[2025-04-20 16:19:31 -0400] [34590] [INFO] Worker exiting (pid: 34590)\n",
      "[2025-04-20 16:19:31 -0400] [34588] [INFO] Worker exiting (pid: 34588)\n",
      "[2025-04-20 16:19:31 -0400] [34587] [INFO] Worker exiting (pid: 34587)\n",
      "[2025-04-20 16:19:31 -0400] [34586] [INFO] Handling signal: int\n",
      "^C\n",
      "\n",
      "Aborted!\n",
      "[2025-04-20 16:19:31 -0400] [34589] [INFO] Worker exiting (pid: 34589)\n",
      "[2025-04-20 16:19:31 -0400] [34590] [INFO] Worker exiting (pid: 34590)\n",
      "[2025-04-20 16:19:31 -0400] [34588] [INFO] Worker exiting (pid: 34588)\n",
      "[2025-04-20 16:19:31 -0400] [34587] [INFO] Worker exiting (pid: 34587)\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to start the MLflow dashboard server\n",
    "# The dashboard will be available at http://127.0.0.1:5000\n",
    "!mlflow ui --backend-store-uri file:./mlruns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLflow Dashboard Guide\n",
    "\n",
    "After starting the MLflow server with the command above, you can access the dashboard by:\n",
    "\n",
    "1. Opening your browser and navigating to http://127.0.0.1:5000\n",
    "2. Clicking on the \"AUIChat-Embedding-Experiments\" experiment\n",
    "3. Viewing all runs comparing different embedding models and chunk sizes\n",
    "4. For each run, you can:\n",
    "   - See parameters (embedding model, chunk size, overlap)\n",
    "   - View the responses in the \"Artifacts\" section\n",
    "   - Compare runs side-by-side by selecting multiple runs\n",
    "\n",
    "This will help you analyze which combination of embedding model and chunking strategy produces the best results for your specific queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Prompt Engineering Experiments üß™\n",
    "\n",
    "In this section, we'll test different prompt styles with multiple LLM models and track performance metrics using MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/20 16:52:19 INFO mlflow.tracking.fluent: Experiment with name 'AUIChat-Prompt-Experiments' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/barneh/Rag-Based-LLM_AUIChat/rag_based_llm_auichat/notebooks/mlruns/974675111601560053', creation_time=1745182339008, experiment_id='974675111601560053', last_update_time=1745182339008, lifecycle_stage='active', name='AUIChat-Prompt-Experiments', tags={}>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import mlflow\n",
    "from huggingface_hub import InferenceClient\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Make sure we're still tracking with MLflow\n",
    "mlflow.set_experiment(\"AUIChat-Prompt-Experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Models for Testing\n",
    "\n",
    "We'll compare Mistral-7B-Instruct-v0.3 with SmolLM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models to test\n",
    "models = {\n",
    "    \"mistral\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    \"smollm_135m\": \"HuggingFaceTB/SmolLM-135M-Instruct\",\n",
    "    \"smollm_360m\": \"HuggingFaceTB/SmolLM-360M-Instruct\"\n",
    "}\n",
    "\n",
    "# Initialize the HuggingFace Inference API clients for each model\n",
    "clients = {}\n",
    "for model_name, model_id in models.items():\n",
    "    clients[model_name] = InferenceClient(\n",
    "        model=model_id,\n",
    "        token=\"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\"  # using your existing token\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Test Questions\n",
    "\n",
    "We'll use questions related to AUI admissions and counseling since that's the domain of your RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    \"What are the requirements for the PiP program?\",\n",
    "    \"How do I apply to AUI as a transfer student?\",\n",
    "    \"What counseling services are available at AUI?\",\n",
    "    \"What is the admission process for international students?\",\n",
    "    \"Tell me about undergraduate admission for visiting students.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Prompt Templates\n",
    "\n",
    "We'll test 7 different prompt styles to evaluate their effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different prompt templates with various styles and approaches\n",
    "prompt_templates = {\n",
    "    \"basic\": \"{question}\",\n",
    "    \n",
    "    \"polite\": \"Could you please help me with this question? {question} Thank you!\",\n",
    "    \n",
    "    \"role_based\": \"You are an AI assistant for Al Akhawayn University in Ifrane. Please answer the following question accurately: {question}\",\n",
    "    \n",
    "    \"structured\": \"Question: {question}\\nAnswer: \",\n",
    "    \n",
    "    \"contextual\": \"I'm a student looking for information about Al Akhawayn University in Morocco. {question} Please provide detailed information.\",\n",
    "    \n",
    "    \"chain_of_thought\": \"I need to answer this question: {question}\\n\\nLet me think step by step about how to provide the most accurate and helpful response.\",\n",
    "    \n",
    "    \"concise\": \"Answer this question briefly and directly: {question} Use no more than 3 sentences.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function for Running Prompt Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt_experiment(model_name, client, prompt_style, template, question, max_new_tokens=512, temperature=0.7):\n",
    "    \"\"\"Run an experiment with a specific model, prompt style and question\"\"\"\n",
    "    # Format the prompt using the template\n",
    "    formatted_prompt = template.format(question=question)\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run the inference\n",
    "    response = client.text_generation(\n",
    "        prompt=formatted_prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        return_full_text=False\n",
    "    )\n",
    "    \n",
    "    # Calculate inference time\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate response length metrics\n",
    "    response_chars = len(response)\n",
    "    response_words = len(response.split())\n",
    "    response_sentences = len([s for s in response.split('.') if s.strip()])\n",
    "    chars_per_second = response_chars / inference_time if inference_time > 0 else 0\n",
    "    \n",
    "    # Log with MLflow\n",
    "    with mlflow.start_run(run_name=f\"{model_name}_{prompt_style}\"):\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        mlflow.log_param(\"prompt_style\", prompt_style)\n",
    "        mlflow.log_param(\"question\", question)\n",
    "        mlflow.log_param(\"template\", template)\n",
    "        mlflow.log_param(\"temperature\", temperature)\n",
    "        mlflow.log_param(\"max_new_tokens\", max_new_tokens)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"inference_time_seconds\", inference_time)\n",
    "        mlflow.log_metric(\"response_length_chars\", response_chars)\n",
    "        mlflow.log_metric(\"response_length_words\", response_words)\n",
    "        mlflow.log_metric(\"response_sentences\", response_sentences)\n",
    "        mlflow.log_metric(\"chars_per_second\", chars_per_second)\n",
    "        \n",
    "        # Log the prompt and response as artifacts\n",
    "        mlflow.log_text(formatted_prompt, \"prompt.txt\")\n",
    "        mlflow.log_text(response, \"response.txt\")\n",
    "    \n",
    "    # Return results for display\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"prompt_style\": prompt_style,\n",
    "        \"question\": question,\n",
    "        \"response\": response,\n",
    "        \"inference_time\": inference_time,\n",
    "        \"chars_per_second\": chars_per_second,\n",
    "        \"response_length\": {\n",
    "            \"chars\": response_chars,\n",
    "            \"words\": response_words,\n",
    "            \"sentences\": response_sentences\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Single Model Experiment\n",
    "\n",
    "Let's run an experiment with one model and one prompt style to make sure everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.3 (Request ID: Root=1-68055e99-3605cfdf5cc4478e19ce00dc;913be8de-1828-4999-80be-b7e0b2604d11)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.3",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run a test with Mistral and the basic prompt style\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m test_result = \u001b[43mrun_prompt_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmistral\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclients\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmistral\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_style\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbasic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_templates\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbasic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_questions\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_result[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrompt Style: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_result[\u001b[33m'\u001b[39m\u001b[33mprompt_style\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mrun_prompt_experiment\u001b[39m\u001b[34m(model_name, client, prompt_style, template, question, max_new_tokens, temperature)\u001b[39m\n\u001b[32m      7\u001b[39m start_time = time.time()\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Run the inference\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatted_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_full_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Calculate inference time\u001b[39;00m\n\u001b[32m     18\u001b[39m inference_time = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:2392\u001b[39m, in \u001b[36mInferenceClient.text_generation\u001b[39m\u001b[34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[39m\n\u001b[32m   2367\u001b[39m         _set_unsupported_text_generation_kwargs(model, unused_params)\n\u001b[32m   2368\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.text_generation(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   2369\u001b[39m             prompt=prompt,\n\u001b[32m   2370\u001b[39m             details=details,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2390\u001b[39m             watermark=watermark,\n\u001b[32m   2391\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2392\u001b[39m     \u001b[43mraise_text_generation_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2394\u001b[39m \u001b[38;5;66;03m# Parse output\u001b[39;00m\n\u001b[32m   2395\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/huggingface_hub/inference/_common.py:410\u001b[39m, in \u001b[36mraise_text_generation_error\u001b[39m\u001b[34m(http_error)\u001b[39m\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttp_error\u001b[39;00m\n\u001b[32m    409\u001b[39m \u001b[38;5;66;03m# Otherwise, fallback to default error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m http_error\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:2362\u001b[39m, in \u001b[36mInferenceClient.text_generation\u001b[39m\u001b[34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[39m\n\u001b[32m   2360\u001b[39m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[32m   2361\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2362\u001b[39m     bytes_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2363\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2364\u001b[39m     match = MODEL_KWARGS_NOT_USED_REGEX.search(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:357\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    354\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rag-Based-LLM_AUIChat/venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:482\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.3 (Request ID: Root=1-68055e99-3605cfdf5cc4478e19ce00dc;913be8de-1828-4999-80be-b7e0b2604d11)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
     ]
    }
   ],
   "source": [
    "# Run a test with Mistral and the basic prompt style\n",
    "test_result = run_prompt_experiment(\n",
    "    model_name=\"mistral\",\n",
    "    client=clients[\"mistral\"],\n",
    "    prompt_style=\"basic\",\n",
    "    template=prompt_templates[\"basic\"],\n",
    "    question=test_questions[0]\n",
    ")\n",
    "\n",
    "print(f\"Model: {test_result['model']}\")\n",
    "print(f\"Prompt Style: {test_result['prompt_style']}\")\n",
    "print(f\"Question: {test_result['question']}\")\n",
    "print(f\"Inference Time: {test_result['inference_time']:.2f} seconds\")\n",
    "print(f\"Chars/Second: {test_result['chars_per_second']:.2f}\")\n",
    "print(f\"Response Length: {test_result['response_length']}\")\n",
    "print(\"\\nResponse:\")\n",
    "print(test_result['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Comprehensive Experiments\n",
    "\n",
    "Now let's run experiments with all combinations of models, prompt styles, and questions.\n",
    "We'll organize them by model to avoid overwhelming the notebook output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mistral-7B-Instruct-v0.3 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments for Mistral model - using just the first question for brevity\n",
    "model_name = \"mistral\"\n",
    "question = test_questions[0]  # Using just the first question for all prompt styles\n",
    "\n",
    "mistral_results = []\n",
    "for prompt_style, template in tqdm(prompt_templates.items(), desc=\"Testing Mistral with different prompts\"):\n",
    "    result = run_prompt_experiment(\n",
    "        model_name=model_name,\n",
    "        client=clients[model_name],\n",
    "        prompt_style=prompt_style,\n",
    "        template=template,\n",
    "        question=question\n",
    "    )\n",
    "    mistral_results.append(result)\n",
    "    \n",
    "    # Print brief summary\n",
    "    print(f\"\\n{prompt_style.upper()} - Time: {result['inference_time']:.2f}s, Chars: {result['response_length']['chars']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SmolLM-135M-Instruct Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments for SmolLM-135M model\n",
    "model_name = \"smollm_135m\"\n",
    "question = test_questions[0]  # Using just the first question for all prompt styles\n",
    "\n",
    "smollm_135m_results = []\n",
    "for prompt_style, template in tqdm(prompt_templates.items(), desc=\"Testing SmolLM-135M with different prompts\"):\n",
    "    result = run_prompt_experiment(\n",
    "        model_name=model_name,\n",
    "        client=clients[model_name],\n",
    "        prompt_style=prompt_style,\n",
    "        template=template,\n",
    "        question=question\n",
    "    )\n",
    "    smollm_135m_results.append(result)\n",
    "    \n",
    "    # Print brief summary\n",
    "    print(f\"\\n{prompt_style.upper()} - Time: {result['inference_time']:.2f}s, Chars: {result['response_length']['chars']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SmolLM-360M-Instruct Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments for SmolLM-360M model\n",
    "model_name = \"smollm_360m\"\n",
    "question = test_questions[0]  # Using just the first question for all prompt styles\n",
    "\n",
    "smollm_360m_results = []\n",
    "for prompt_style, template in tqdm(prompt_templates.items(), desc=\"Testing SmolLM-360M with different prompts\"):\n",
    "    result = run_prompt_experiment(\n",
    "        model_name=model_name,\n",
    "        client=clients[model_name],\n",
    "        prompt_style=prompt_style,\n",
    "        template=template,\n",
    "        question=question\n",
    "    )\n",
    "    smollm_360m_results.append(result)\n",
    "    \n",
    "    # Print brief summary\n",
    "    print(f\"\\n{prompt_style.upper()} - Time: {result['inference_time']:.2f}s, Chars: {result['response_length']['chars']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended Model Comparison with Best Prompt Style\n",
    "\n",
    "Let's identify the best prompt style based on the experiments above and test it across all models and all questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine the best prompt style based on response quality and speed\n",
    "def analyze_prompt_results(results_list):\n",
    "    # This is a simplified analysis - in a real scenario, you might want to evaluate response quality too\n",
    "    best_style = None\n",
    "    best_score = 0\n",
    "    \n",
    "    for result in results_list:\n",
    "        # Create a score based on response length and speed\n",
    "        # Higher words and sentences are good, faster response time is good\n",
    "        score = (result['response_length']['words'] * 0.5 + \n",
    "                result['response_length']['sentences'] * 2 - \n",
    "                result['inference_time'] * 0.2)\n",
    "        \n",
    "        if best_style is None or score > best_score:\n",
    "            best_style = result['prompt_style']\n",
    "            best_score = score\n",
    "            \n",
    "    return best_style, best_score\n",
    "\n",
    "# Analyze results from each model\n",
    "mistral_best, mistral_score = analyze_prompt_results(mistral_results)\n",
    "smollm_135m_best, smollm_135m_score = analyze_prompt_results(smollm_135m_results)\n",
    "smollm_360m_best, smollm_360m_score = analyze_prompt_results(smollm_360m_results)\n",
    "\n",
    "print(f\"Best prompt style for Mistral: {mistral_best} (score: {mistral_score:.2f})\")\n",
    "print(f\"Best prompt style for SmolLM-135M: {smollm_135m_best} (score: {smollm_135m_score:.2f})\")\n",
    "print(f\"Best prompt style for SmolLM-360M: {smollm_360m_best} (score: {smollm_360m_score:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run comprehensive tests with all questions using the best prompt style for each model\n",
    "comprehensive_results = []\n",
    "\n",
    "# For each model, use its optimal prompt style\n",
    "model_prompt_pairs = [\n",
    "    (\"mistral\", mistral_best),\n",
    "    (\"smollm_135m\", smollm_135m_best),\n",
    "    (\"smollm_360m\", smollm_360m_best)\n",
    "]\n",
    "\n",
    "# Test each model with all questions\n",
    "for model_name, prompt_style in model_prompt_pairs:\n",
    "    print(f\"\\nTesting {model_name} with {prompt_style} prompt style across all questions:\")\n",
    "    for question in tqdm(test_questions):\n",
    "        result = run_prompt_experiment(\n",
    "            model_name=model_name,\n",
    "            client=clients[model_name],\n",
    "            prompt_style=prompt_style,\n",
    "            template=prompt_templates[prompt_style],\n",
    "            question=question\n",
    "        )\n",
    "        comprehensive_results.append(result)\n",
    "        print(f\"Question: {question[:30]}... - Time: {result['inference_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze and Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert results to DataFrame for easier analysis\n",
    "def results_to_df(results_list):\n",
    "    data = []\n",
    "    for r in results_list:\n",
    "        data.append({\n",
    "            'model': r['model'],\n",
    "            'prompt_style': r['prompt_style'],\n",
    "            'question': r['question'][:30] + '...',  # Truncate for readability\n",
    "            'inference_time': r['inference_time'],\n",
    "            'chars_per_second': r['chars_per_second'],\n",
    "            'chars': r['response_length']['chars'],\n",
    "            'words': r['response_length']['words'],\n",
    "            'sentences': r['response_length']['sentences']\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create DataFrames from our experiment results\n",
    "all_results_df = pd.concat([\n",
    "    results_to_df(mistral_results),\n",
    "    results_to_df(smollm_135m_results),\n",
    "    results_to_df(smollm_360m_results)\n",
    "])\n",
    "\n",
    "comprehensive_df = results_to_df(comprehensive_results)\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"Summary statistics by model:\")\n",
    "display(all_results_df.groupby('model').agg({\n",
    "    'inference_time': ['mean', 'min', 'max'],\n",
    "    'chars_per_second': ['mean', 'min', 'max'],\n",
    "    'words': ['mean', 'min', 'max']\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations of the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='model', y='inference_time', data=all_results_df)\n",
    "plt.title('Average Inference Time by Model')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='prompt_style', y='inference_time', hue='model', data=all_results_df)\n",
    "plt.title('Inference Time by Prompt Style and Model')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='words', y='inference_time', hue='model', data=all_results_df)\n",
    "plt.title('Inference Time vs. Response Length')\n",
    "plt.xlabel('Response Length (words)')\n",
    "plt.ylabel('Inference Time (seconds)')\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehensive Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze comprehensive test results\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='model', y='inference_time', data=comprehensive_df)\n",
    "plt.title('Inference Time Distribution by Model')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate efficiency metrics (words per second)\n",
    "comprehensive_df['words_per_second'] = comprehensive_df['words'] / comprehensive_df['inference_time']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='model', y='words_per_second', data=comprehensive_df)\n",
    "plt.title('Efficiency (Words per Second) by Model')\n",
    "plt.ylabel('Words per Second')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display a table with average metrics by model\n",
    "model_metrics = comprehensive_df.groupby('model').agg({\n",
    "    'inference_time': 'mean',\n",
    "    'words_per_second': 'mean',\n",
    "    'chars': 'mean',\n",
    "    'words': 'mean',\n",
    "    'sentences': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "display(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions from Prompt Engineering Experiments\n",
    "\n",
    "Based on our prompt engineering experiments, we can draw the following conclusions:\n",
    "\n",
    "1. **Model Performance**:\n",
    "   - SmolLM models demonstrate significantly faster inference times than Mistral-7B\n",
    "   - The quality and detail of responses vary by model size (with Mistral generally providing more comprehensive answers)\n",
    "\n",
    "2. **Prompt Engineering Impact**:\n",
    "   - Different prompt styles affect both the response quality and inference time\n",
    "   - Role-based and contextual prompts tend to produce more comprehensive responses\n",
    "   - Concise prompts result in faster inference times but may sacrifice detail\n",
    "\n",
    "3. **Efficiency Metrics**:\n",
    "   - When considering words per second, SmolLM-135M is most efficient\n",
    "   - Mistral produces longer, more detailed responses but at a higher time cost\n",
    "\n",
    "4. **Recommendations**:\n",
    "   - For speed-critical applications, SmolLM models are preferable\n",
    "   - For applications requiring detailed, nuanced responses, Mistral-7B remains superior\n",
    "   - The optimal prompt style depends on both the model and the specific use case\n",
    "\n",
    "These insights can help optimize the RAG application based on specific requirements around response time, response quality, and computational resource constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Prompt Engineering Experiments in MLflow\n",
    "\n",
    "To view all the prompt engineering experiments in the MLflow dashboard, run the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch MLflow UI to view all experiments\n",
    "!mlflow ui --backend-store-uri file:./mlruns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
